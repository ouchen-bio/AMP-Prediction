{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ouchen-bio/AMP-Prediction/blob/main/AMP_SKL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCOSm4IUyUj0",
        "outputId": "ea85e828-703f-4470-cf36-d1b0870c5b86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-11-20 15:45:00--  https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85055499 (81M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-py37_4.8.2-Linux-x86_64.sh’\n",
            "\n",
            "Miniconda3-py37_4.8 100%[===================>]  81.12M   127MB/s    in 0.6s    \n",
            "\n",
            "2022-11-20 15:45:00 (127 MB/s) - ‘Miniconda3-py37_4.8.2-Linux-x86_64.sh’ saved [85055499/85055499]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - asn1crypto==1.3.0=py37_0\n",
            "    - ca-certificates==2020.1.1=0\n",
            "    - certifi==2019.11.28=py37_0\n",
            "    - cffi==1.14.0=py37h2e261b9_0\n",
            "    - chardet==3.0.4=py37_1003\n",
            "    - conda-package-handling==1.6.0=py37h7b6447c_0\n",
            "    - conda==4.8.2=py37_0\n",
            "    - cryptography==2.8=py37h1ba5d50_0\n",
            "    - idna==2.8=py37_0\n",
            "    - ld_impl_linux-64==2.33.1=h53a641e_7\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.2.1=hd88cf55_4\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.2=he6710b0_0\n",
            "    - openssl==1.1.1d=h7b6447c_4\n",
            "    - pip==20.0.2=py37_1\n",
            "    - pycosat==0.6.3=py37h7b6447c_0\n",
            "    - pycparser==2.19=py37_0\n",
            "    - pyopenssl==19.1.0=py37_0\n",
            "    - pysocks==1.7.1=py37_0\n",
            "    - python==3.7.6=h0371630_2\n",
            "    - readline==7.0=h7b6447c_5\n",
            "    - requests==2.22.0=py37_1\n",
            "    - ruamel_yaml==0.15.87=py37h7b6447c_0\n",
            "    - setuptools==45.2.0=py37_0\n",
            "    - six==1.14.0=py37_0\n",
            "    - sqlite==3.31.1=h7b6447c_0\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.42.1=py_0\n",
            "    - urllib3==1.25.8=py37_0\n",
            "    - wheel==0.34.2=py37_0\n",
            "    - xz==5.2.4=h14c3975_4\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  asn1crypto         pkgs/main/linux-64::asn1crypto-1.3.0-py37_0\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2020.1.1-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2019.11.28-py37_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.14.0-py37h2e261b9_0\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py37_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.8.2-py37_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.6.0-py37h7b6447c_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-2.8-py37h1ba5d50_0\n",
            "  idna               pkgs/main/linux-64::idna-2.8-py37_0\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.33.1-h53a641e_7\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20181209-hc058e9b_0\n",
            "  libffi             pkgs/main/linux-64::libffi-3.2.1-hd88cf55_4\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_0\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1d-h7b6447c_4\n",
            "  pip                pkgs/main/linux-64::pip-20.0.2-py37_1\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h7b6447c_0\n",
            "  pycparser          pkgs/main/linux-64::pycparser-2.19-py37_0\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.1.0-py37_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py37_0\n",
            "  python             pkgs/main/linux-64::python-3.7.6-h0371630_2\n",
            "  readline           pkgs/main/linux-64::readline-7.0-h7b6447c_5\n",
            "  requests           pkgs/main/linux-64::requests-2.22.0-py37_1\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.87-py37h7b6447c_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-45.2.0-py37_0\n",
            "  six                pkgs/main/linux-64::six-1.14.0-py37_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.31.1-h7b6447c_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.8-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.42.1-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.25.8-py37_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.34.2-py37_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.4-h14c3975_4\n",
            "  yaml               pkgs/main/linux-64::yaml-0.1.7-had09818_2\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "# INSTALL CONDA ON GOOGLE COLAB\n",
        "################################################################################\n",
        "! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
        "! chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
        "! bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
      ],
      "id": "hCOSm4IUyUj0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUYYb_RLzJEW",
        "outputId": "87534f47-80a5-475b-adfc-4ee2e123a131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - cd-hit\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2022.10.11 |       h06a4308_0         124 KB\n",
            "    cd-hit-4.8.1               |       hdbcaa40_0         221 KB  bioconda\n",
            "    certifi-2022.9.24          |   py37h06a4308_0         154 KB\n",
            "    conda-22.9.0               |   py37h06a4308_0         878 KB\n",
            "    openssl-1.1.1s             |       h7f8727e_0         3.6 MB\n",
            "    toolz-0.12.0               |   py37h06a4308_0         104 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         5.0 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  cd-hit             bioconda/linux-64::cd-hit-4.8.1-hdbcaa40_0\n",
            "  toolz              pkgs/main/linux-64::toolz-0.12.0-py37h06a4308_0\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates                                2020.1.1-0 --> 2022.10.11-h06a4308_0\n",
            "  certifi                                 2019.11.28-py37_0 --> 2022.9.24-py37h06a4308_0\n",
            "  conda                                        4.8.2-py37_0 --> 22.9.0-py37h06a4308_0\n",
            "  openssl                                 1.1.1d-h7b6447c_4 --> 1.1.1s-h7f8727e_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "ca-certificates-2022 | 124 KB    | : 100% 1.0/1 [00:00<00:00,  7.27it/s]\n",
            "toolz-0.12.0         | 104 KB    | : 100% 1.0/1 [00:00<00:00, 10.26it/s]\n",
            "certifi-2022.9.24    | 154 KB    | : 100% 1.0/1 [00:00<00:00, 13.10it/s]\n",
            "cd-hit-4.8.1         | 221 KB    | : 100% 1.0/1 [00:00<00:00,  2.65it/s]                \n",
            "conda-22.9.0         | 878 KB    | : 100% 1.0/1 [00:00<00:00,  6.42it/s]\n",
            "openssl-1.1.1s       | 3.6 MB    | : 100% 1.0/1 [00:00<00:00,  5.37it/s]\n",
            "Preparing transaction: | \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\bdone\n"
          ]
        }
      ],
      "source": [
        "! conda install -c bioconda cd-hit -y"
      ],
      "id": "mUYYb_RLzJEW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am1d3QaFzdk-",
        "outputId": "9b047ba4-1d06-4562-e882-bbffdbdbdcc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================\n",
            "Program: CD-HIT, V4.8.1, Mar 01 2019, 14:14:47\n",
            "Command: cd-hit -i train_pn.fasta -o train_pn_cdhit.txt -c\n",
            "         0.99\n",
            "\n",
            "Started: Sun Nov 20 15:45:40 2022\n",
            "================================================================\n",
            "                            Output                              \n",
            "----------------------------------------------------------------\n",
            "\n",
            "Fatal Error:\n",
            "Failed to open the database file\n",
            "Program halted !!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! cd-hit -i train_pn.fasta -o train_pn_cdhit.txt -c 0.99"
      ],
      "id": "Am1d3QaFzdk-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG-QdmIS0VwA",
        "outputId": "8e9655b8-6424-4b2a-cdcf-8d503a93558f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================\n",
            "Program: CD-HIT, V4.8.1, Mar 01 2019, 14:14:47\n",
            "Command: cd-hit -i train_ne.fasta -o train_ne_cdhit.fasta -c\n",
            "         0.99\n",
            "\n",
            "Started: Sun Nov 20 15:48:22 2022\n",
            "================================================================\n",
            "                            Output                              \n",
            "----------------------------------------------------------------\n",
            "total seq: 1445\n",
            "longest and shortest : 30 and 11\n",
            "Total letters: 30071\n",
            "Sequences have been sorted\n",
            "\n",
            "Approximated minimal memory consumption:\n",
            "Sequence        : 0M\n",
            "Buffer          : 1 X 10M = 10M\n",
            "Table           : 1 X 65M = 65M\n",
            "Miscellaneous   : 0M\n",
            "Total           : 76M\n",
            "\n",
            "Table limit with the given memory limit:\n",
            "Max number of representatives: 4000000\n",
            "Max number of word counting entries: 90485331\n",
            "\n",
            "\rcomparing sequences from          0  to       1445\n",
            ".\n",
            "     1445  finished       1422  clusters\n",
            "\n",
            "Approximated maximum memory consumption: 76M\n",
            "writing new database\n",
            "writing clustering information\n",
            "program completed !\n",
            "\n",
            "Total CPU time 0.08\n"
          ]
        }
      ],
      "source": [
        "! cd-hit -i train_po.fasta -o test__cdhit.txt -c 0.99"
      ],
      "id": "EG-QdmIS0VwA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h77VW4zG0fON",
        "outputId": "93a36029-7b0d-4fd2-c47a-6becdb8a638e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================\n",
            "Program: CD-HIT, V4.8.1, Mar 01 2019, 14:14:47\n",
            "Command: cd-hit -i test_po.fasta -o test_po_cdhit.txt -c 0.99\n",
            "\n",
            "Started: Sat Nov 19 16:55:43 2022\n",
            "================================================================\n",
            "                            Output                              \n",
            "----------------------------------------------------------------\n",
            "total seq: 94\n",
            "longest and shortest : 30 and 11\n",
            "Total letters: 1793\n",
            "Sequences have been sorted\n",
            "\n",
            "Approximated minimal memory consumption:\n",
            "Sequence        : 0M\n",
            "Buffer          : 1 X 10M = 10M\n",
            "Table           : 1 X 65M = 65M\n",
            "Miscellaneous   : 0M\n",
            "Total           : 75M\n",
            "\n",
            "Table limit with the given memory limit:\n",
            "Max number of representatives: 4000000\n",
            "Max number of word counting entries: 90517567\n",
            "\n",
            "\rcomparing sequences from          0  to         94\n",
            "\n",
            "       94  finished         94  clusters\n",
            "\n",
            "Approximated maximum memory consumption: 75M\n",
            "writing new database\n",
            "writing clustering information\n",
            "program completed !\n",
            "\n",
            "Total CPU time 0.07\n"
          ]
        }
      ],
      "source": [
        "! cd-hit -i test_po.fasta -o test_po_cdhit.txt -c 0.99"
      ],
      "id": "h77VW4zG0fON"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-ylzrIMfvIH",
        "outputId": "36230538-5bf7-4d2c-a82c-cd2f0ac6a896"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.80-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 2.4 MB/s \n",
            "\u001b[?25hCollecting numpy\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 237 kB/s \n",
            "\u001b[?25hInstalling collected packages: numpy, biopython\n",
            "Successfully installed biopython-1.80 numpy-1.21.6\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 4.0 MB/s \n",
            "\u001b[?25hCollecting pytz>=2017.3\n",
            "  Downloading pytz-2022.6-py2.py3-none-any.whl (498 kB)\n",
            "\u001b[K     |████████████████████████████████| 498 kB 69.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /usr/local/lib/python3.7/site-packages (from pandas) (1.21.6)\n",
            "Collecting python-dateutil>=2.7.3\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 57.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\n",
            "Installing collected packages: pytz, python-dateutil, pandas\n",
            "Successfully installed pandas-1.3.5 python-dateutil-2.8.2 pytz-2022.6\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting peptides\n",
            "  Downloading peptides-0.3.1-py3-none-any.whl (110 kB)\n",
            "\u001b[K     |████████████████████████████████| 110 kB 4.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: peptides\n",
            "Successfully installed peptides-0.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 588.3 MB 6.4 kB/s \n"
          ]
        }
      ],
      "source": [
        "!pip install biopython\n",
        "!pip install pandas\n",
        "!pip install peptides\n",
        "!pip install tensorflow \n",
        "!pip install keras \n",
        "!pip install matplotlib"
      ],
      "id": "A-ylzrIMfvIH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4b018f74"
      },
      "outputs": [],
      "source": [
        "from Bio import SeqIO\n",
        "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
        "import pandas as pd\n",
        "import peptides"
      ],
      "id": "4b018f74"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fccb297e"
      },
      "outputs": [],
      "source": [
        "def load_train(train):\n",
        "    train_list=[]\n",
        "    for record in train:\n",
        "        amp_id=record.id\n",
        "        amp_seq=record.seq\n",
        "        peptide=peptides.Peptide(str(amp_seq))\n",
        "        aliph_index=round(peptide.aliphatic_index(),2)\n",
        "        boman=round(peptide.boman(),2)\n",
        "        charge=round(peptide.charge(pKscale=\"Murray\"),2)\n",
        "        amp_size=len(amp_seq)\n",
        "        amp_analysed=ProteinAnalysis(str(amp_seq))\n",
        "        MW=float(\"%0.2f\" % amp_analysed.molecular_weight())\n",
        "        A=round(amp_analysed.get_amino_acids_percent()['A'],2)\n",
        "        R=round(amp_analysed.get_amino_acids_percent()['R'],2)\n",
        "        N=round(amp_analysed.get_amino_acids_percent()['N'],2)\n",
        "        D=round(amp_analysed.get_amino_acids_percent()['D'],2)\n",
        "        C=round(amp_analysed.get_amino_acids_percent()['C'],2)\n",
        "        E=round(amp_analysed.get_amino_acids_percent()['E'],2)\n",
        "        Q=round(amp_analysed.get_amino_acids_percent()['Q'],2)\n",
        "        G=round(amp_analysed.get_amino_acids_percent()['G'],2)\n",
        "        H=round(amp_analysed.get_amino_acids_percent()['H'],2)\n",
        "        I=round(amp_analysed.get_amino_acids_percent()['I'],2)\n",
        "        L=round(amp_analysed.get_amino_acids_percent()['L'],2)\n",
        "        K=round(amp_analysed.get_amino_acids_percent()['K'],2)\n",
        "        M=round(amp_analysed.get_amino_acids_percent()['M'],2)\n",
        "        F=round(amp_analysed.get_amino_acids_percent()['F'],2)\n",
        "        P=round(amp_analysed.get_amino_acids_percent()['P'],2)\n",
        "        S=round(amp_analysed.get_amino_acids_percent()['S'],2)\n",
        "        T=round(amp_analysed.get_amino_acids_percent()['T'],2)\n",
        "        W=round(amp_analysed.get_amino_acids_percent()['W'],2)\n",
        "        Y=round(amp_analysed.get_amino_acids_percent()['Y'],2)\n",
        "        V=round(amp_analysed.get_amino_acids_percent()['V'],2)\n",
        "        IP= float(\"%0.2f\" % amp_analysed.isoelectric_point()) #isoelectric_point\n",
        "        ARM=float(\"%0.2f\" % amp_analysed.aromaticity())#aromaticity\n",
        "        INS=float(\"%0.2f\" % amp_analysed.instability_index()) #instability_index (instability in vitro): < 40==> stable, >= 40 ==> instable \n",
        "        GV=float(\"%0.2f\" % amp_analysed.gravy()) #(grand average of hydropathy) above 0 ==> hydrophobic, beleow 0 ==> hydrophilic\n",
        "        # Secondary Stracture Fractions\n",
        "        amp_SSF=amp_analysed.secondary_structure_fraction()\n",
        "        Helix=float(\"%0.2f\" % amp_SSF[0])\n",
        "        Turn=float(\"%0.2f\" % amp_SSF[1])\n",
        "        Sheet=float(\"%0.2f\" % amp_SSF[2])\n",
        "        #molar extinction coefficient\n",
        "        amp_MEC=amp_analysed.molar_extinction_coefficient()\n",
        "        reduced_cysteines=amp_MEC[0] #reduced_cysteines\n",
        "        cystines_residues=amp_MEC[1] #cystines_residues cys-cys bound\n",
        "        # instability\n",
        "        if float(INS) < 40:\n",
        "            Instability=1 # likely to be stable in essay tube\n",
        "        else:\n",
        "            Instability=0 #likely to be instable in essay tube\n",
        "        # hydropathy\n",
        "        if GV < 0:\n",
        "            Hydrophobicity=0 #hydrophilic\n",
        "        else:\n",
        "            Hydrophobicity=1 #hydrophobic\n",
        "        # AMP Activity\n",
        "        if 'non' in amp_id:\n",
        "            amp_activity=0 #negative\n",
        "        else:\n",
        "            amp_activity=1 #positive\n",
        "        \n",
        "        amp_phy_ch=(A,R,N,D,C,E,Q,G,H,I,L,K,M,F,P,S,T,W,Y,V,Helix,Turn,Sheet,aliph_index,boman,MW,reduced_cysteines,cystines_residues,IP,charge,ARM,Instability,Hydrophobicity,amp_activity)\n",
        "        train_list.append(amp_phy_ch)\n",
        "    train_df=pd.DataFrame.from_records(train_list,columns=['A','R','N','D','C','E','Q','G','H','I','L','K','M','F','P','S','T','W','Y','V','Helix','Turn','Sheet','aliphatic_index','boman_index','Molecular Weight','MEC RC','MEC SS','Isoelectric Point','Charge','Aromaticity','Instability','Hydrophobicity','AMP Activity'])\n",
        "    return train_df"
      ],
      "id": "fccb297e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21ad8eb4"
      },
      "outputs": [],
      "source": [
        "def descr_train(train):\n",
        "    train_descp=[]\n",
        "    for record in train:\n",
        "        amp_id=record.id\n",
        "        amp_seq=record.seq\n",
        "        peptide=peptides.Peptide(str(amp_seq))\n",
        "        descriptors=peptide.descriptors() \n",
        "        train_descp.append(descriptors)    \n",
        "    return train_descp"
      ],
      "id": "21ad8eb4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6c08374"
      },
      "outputs": [],
      "source": [
        "train_path='train_pn.fasta'\n",
        "train_fasta=SeqIO.parse(train_path,'fasta')"
      ],
      "id": "b6c08374"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "97387138",
        "outputId": "98d359ad-ac76-4bae-bd89-1061131e91a3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-baba7536-5094-4b2d-9737-581c05d1b453\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>R</th>\n",
              "      <th>N</th>\n",
              "      <th>D</th>\n",
              "      <th>C</th>\n",
              "      <th>E</th>\n",
              "      <th>Q</th>\n",
              "      <th>G</th>\n",
              "      <th>H</th>\n",
              "      <th>I</th>\n",
              "      <th>...</th>\n",
              "      <th>boman_index</th>\n",
              "      <th>Molecular Weight</th>\n",
              "      <th>MEC RC</th>\n",
              "      <th>MEC SS</th>\n",
              "      <th>Isoelectric Point</th>\n",
              "      <th>Charge</th>\n",
              "      <th>Aromaticity</th>\n",
              "      <th>Instability</th>\n",
              "      <th>Hydrophobicity</th>\n",
              "      <th>AMP Activity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.49</td>\n",
              "      <td>407.44</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.56</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.31</td>\n",
              "      <td>477.58</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.57</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.28</td>\n",
              "      <td>493.57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.57</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.92</td>\n",
              "      <td>567.72</td>\n",
              "      <td>0</td>\n",
              "      <td>125</td>\n",
              "      <td>5.51</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>387.39</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.24</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3053</th>\n",
              "      <td>0.13</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.03</td>\n",
              "      <td>...</td>\n",
              "      <td>1.16</td>\n",
              "      <td>3383.02</td>\n",
              "      <td>16500</td>\n",
              "      <td>16500</td>\n",
              "      <td>10.22</td>\n",
              "      <td>6.00</td>\n",
              "      <td>0.10</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3054</th>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.03</td>\n",
              "      <td>...</td>\n",
              "      <td>0.45</td>\n",
              "      <td>3128.65</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>3.09</td>\n",
              "      <td>0.07</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3055</th>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>...</td>\n",
              "      <td>0.04</td>\n",
              "      <td>3111.72</td>\n",
              "      <td>2980</td>\n",
              "      <td>3355</td>\n",
              "      <td>8.32</td>\n",
              "      <td>1.73</td>\n",
              "      <td>0.07</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3056</th>\n",
              "      <td>0.07</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.03</td>\n",
              "      <td>...</td>\n",
              "      <td>2.63</td>\n",
              "      <td>3561.00</td>\n",
              "      <td>1490</td>\n",
              "      <td>1490</td>\n",
              "      <td>9.98</td>\n",
              "      <td>2.09</td>\n",
              "      <td>0.10</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3057</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>...</td>\n",
              "      <td>0.82</td>\n",
              "      <td>3429.17</td>\n",
              "      <td>1490</td>\n",
              "      <td>1865</td>\n",
              "      <td>5.97</td>\n",
              "      <td>-1.09</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3058 rows × 34 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-baba7536-5094-4b2d-9737-581c05d1b453')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-baba7536-5094-4b2d-9737-581c05d1b453 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-baba7536-5094-4b2d-9737-581c05d1b453');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         A     R     N     D    C     E     Q     G     H     I  ...  \\\n",
              "0     0.40  0.00  0.00  0.00  0.2  0.00  0.00  0.20  0.00  0.00  ...   \n",
              "1     0.20  0.00  0.00  0.00  0.0  0.00  0.00  0.20  0.00  0.00  ...   \n",
              "2     0.20  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  ...   \n",
              "3     0.00  0.00  0.00  0.00  0.4  0.00  0.00  0.00  0.00  0.00  ...   \n",
              "4     0.20  0.00  0.00  0.00  0.0  0.00  0.00  0.40  0.00  0.00  ...   \n",
              "...    ...   ...   ...   ...  ...   ...   ...   ...   ...   ...  ...   \n",
              "3053  0.13  0.00  0.03  0.03  0.0  0.03  0.03  0.13  0.00  0.03  ...   \n",
              "3054  0.07  0.00  0.00  0.00  0.0  0.03  0.10  0.20  0.03  0.03  ...   \n",
              "3055  0.07  0.00  0.03  0.00  0.2  0.03  0.00  0.10  0.00  0.07  ...   \n",
              "3056  0.07  0.10  0.17  0.03  0.0  0.03  0.07  0.00  0.03  0.03  ...   \n",
              "3057  0.00  0.03  0.00  0.07  0.2  0.07  0.00  0.07  0.07  0.07  ...   \n",
              "\n",
              "      boman_index  Molecular Weight  MEC RC  MEC SS  Isoelectric Point  \\\n",
              "0           -0.49            407.44       0       0               5.56   \n",
              "1           -1.31            477.58       0       0               5.57   \n",
              "2           -0.28            493.57       0       0               5.57   \n",
              "3           -1.92            567.72       0     125               5.51   \n",
              "4           -0.06            387.39       0       0               5.24   \n",
              "...           ...               ...     ...     ...                ...   \n",
              "3053         1.16           3383.02   16500   16500              10.22   \n",
              "3054         0.45           3128.65       0       0              10.00   \n",
              "3055         0.04           3111.72    2980    3355               8.32   \n",
              "3056         2.63           3561.00    1490    1490               9.98   \n",
              "3057         0.82           3429.17    1490    1865               5.97   \n",
              "\n",
              "      Charge  Aromaticity  Instability  Hydrophobicity  AMP Activity  \n",
              "0      -0.05         0.00            0               1             1  \n",
              "1      -0.00         0.00            1               1             1  \n",
              "2      -0.00         0.00            0               1             1  \n",
              "3      -0.09         0.20            0               1             1  \n",
              "4      -0.00         0.00            0               0             1  \n",
              "...      ...          ...          ...             ...           ...  \n",
              "3053    6.00         0.10            1               0             0  \n",
              "3054    3.09         0.07            1               0             0  \n",
              "3055    1.73         0.07            1               1             0  \n",
              "3056    2.09         0.10            1               0             0  \n",
              "3057   -1.09         0.03            0               1             0  \n",
              "\n",
              "[3058 rows x 34 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data=load_train(train_fasta)\n",
        "train_data"
      ],
      "id": "97387138"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b9eec10"
      },
      "outputs": [],
      "source": [
        "train_path='train_pn.fasta'\n",
        "train_fasta=SeqIO.parse(train_path,'fasta')\n",
        "descr_data=descr_train(train_fasta)"
      ],
      "id": "3b9eec10"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "f4d7c0f1",
        "outputId": "ccf71efe-534f-44fa-f28f-5516c2b4579a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d2e01624-bb33-4bfa-8560-329c35c04ba2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BLOSUM1</th>\n",
              "      <th>BLOSUM2</th>\n",
              "      <th>BLOSUM3</th>\n",
              "      <th>BLOSUM4</th>\n",
              "      <th>BLOSUM5</th>\n",
              "      <th>BLOSUM6</th>\n",
              "      <th>BLOSUM7</th>\n",
              "      <th>BLOSUM8</th>\n",
              "      <th>BLOSUM9</th>\n",
              "      <th>BLOSUM10</th>\n",
              "      <th>...</th>\n",
              "      <th>VHSE4</th>\n",
              "      <th>VHSE5</th>\n",
              "      <th>VHSE6</th>\n",
              "      <th>VHSE7</th>\n",
              "      <th>VHSE8</th>\n",
              "      <th>Z1</th>\n",
              "      <th>Z2</th>\n",
              "      <th>Z3</th>\n",
              "      <th>Z4</th>\n",
              "      <th>Z5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.154000</td>\n",
              "      <td>-0.646000</td>\n",
              "      <td>0.954000</td>\n",
              "      <td>0.244000</td>\n",
              "      <td>0.392000</td>\n",
              "      <td>0.212000</td>\n",
              "      <td>0.360000</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>0.646000</td>\n",
              "      <td>0.134000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.036000</td>\n",
              "      <td>-0.162000</td>\n",
              "      <td>-0.306000</td>\n",
              "      <td>0.096000</td>\n",
              "      <td>-0.476000</td>\n",
              "      <td>1.152000</td>\n",
              "      <td>-2.288000</td>\n",
              "      <td>1.292000</td>\n",
              "      <td>-0.462000</td>\n",
              "      <td>0.048000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.194000</td>\n",
              "      <td>-0.660000</td>\n",
              "      <td>0.168000</td>\n",
              "      <td>-0.078000</td>\n",
              "      <td>-0.130000</td>\n",
              "      <td>0.458000</td>\n",
              "      <td>0.328000</td>\n",
              "      <td>-0.182000</td>\n",
              "      <td>0.242000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.322000</td>\n",
              "      <td>-0.024000</td>\n",
              "      <td>-0.680000</td>\n",
              "      <td>0.096000</td>\n",
              "      <td>-0.428000</td>\n",
              "      <td>-0.480000</td>\n",
              "      <td>-2.284000</td>\n",
              "      <td>-0.246000</td>\n",
              "      <td>-0.266000</td>\n",
              "      <td>-0.096000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.066000</td>\n",
              "      <td>-0.728000</td>\n",
              "      <td>-0.046000</td>\n",
              "      <td>0.034000</td>\n",
              "      <td>0.056000</td>\n",
              "      <td>0.360000</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>-0.284000</td>\n",
              "      <td>0.458000</td>\n",
              "      <td>0.208000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.730000</td>\n",
              "      <td>-0.034000</td>\n",
              "      <td>-0.334000</td>\n",
              "      <td>-0.404000</td>\n",
              "      <td>-0.194000</td>\n",
              "      <td>-0.084000</td>\n",
              "      <td>-1.464000</td>\n",
              "      <td>0.366000</td>\n",
              "      <td>-0.366000</td>\n",
              "      <td>0.328000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.806000</td>\n",
              "      <td>-0.624000</td>\n",
              "      <td>0.512000</td>\n",
              "      <td>0.286000</td>\n",
              "      <td>0.254000</td>\n",
              "      <td>-0.734000</td>\n",
              "      <td>-0.128000</td>\n",
              "      <td>0.106000</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>-0.236000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.488000</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>-0.012000</td>\n",
              "      <td>-0.996000</td>\n",
              "      <td>0.590000</td>\n",
              "      <td>-1.358000</td>\n",
              "      <td>-0.754000</td>\n",
              "      <td>1.772000</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>-0.788000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.700000</td>\n",
              "      <td>-0.342000</td>\n",
              "      <td>0.958000</td>\n",
              "      <td>-0.074000</td>\n",
              "      <td>-0.380000</td>\n",
              "      <td>0.218000</td>\n",
              "      <td>0.788000</td>\n",
              "      <td>0.108000</td>\n",
              "      <td>0.316000</td>\n",
              "      <td>0.076000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.656000</td>\n",
              "      <td>-0.274000</td>\n",
              "      <td>-0.868000</td>\n",
              "      <td>0.710000</td>\n",
              "      <td>0.102000</td>\n",
              "      <td>1.014000</td>\n",
              "      <td>-2.248000</td>\n",
              "      <td>0.862000</td>\n",
              "      <td>-0.494000</td>\n",
              "      <td>0.642000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3053</th>\n",
              "      <td>0.271667</td>\n",
              "      <td>-0.047667</td>\n",
              "      <td>0.035667</td>\n",
              "      <td>-0.413000</td>\n",
              "      <td>0.118000</td>\n",
              "      <td>0.258333</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.073000</td>\n",
              "      <td>0.110000</td>\n",
              "      <td>-0.073333</td>\n",
              "      <td>...</td>\n",
              "      <td>0.336000</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>-0.265000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>-0.271333</td>\n",
              "      <td>0.193333</td>\n",
              "      <td>-0.367667</td>\n",
              "      <td>-0.596000</td>\n",
              "      <td>0.286333</td>\n",
              "      <td>0.252000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3054</th>\n",
              "      <td>0.187000</td>\n",
              "      <td>-0.166000</td>\n",
              "      <td>-0.031333</td>\n",
              "      <td>-0.120000</td>\n",
              "      <td>-0.024333</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.288667</td>\n",
              "      <td>0.128000</td>\n",
              "      <td>0.158667</td>\n",
              "      <td>0.000667</td>\n",
              "      <td>...</td>\n",
              "      <td>0.233667</td>\n",
              "      <td>0.115333</td>\n",
              "      <td>-0.416333</td>\n",
              "      <td>0.489333</td>\n",
              "      <td>-0.286333</td>\n",
              "      <td>0.058333</td>\n",
              "      <td>-1.056333</td>\n",
              "      <td>-0.479000</td>\n",
              "      <td>-0.249667</td>\n",
              "      <td>0.256667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3055</th>\n",
              "      <td>-0.118000</td>\n",
              "      <td>-0.490333</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.146000</td>\n",
              "      <td>0.228667</td>\n",
              "      <td>-0.102667</td>\n",
              "      <td>0.052667</td>\n",
              "      <td>-0.038000</td>\n",
              "      <td>0.137667</td>\n",
              "      <td>-0.020667</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.144000</td>\n",
              "      <td>0.080667</td>\n",
              "      <td>-0.197333</td>\n",
              "      <td>-0.083333</td>\n",
              "      <td>-0.013333</td>\n",
              "      <td>0.091667</td>\n",
              "      <td>-1.201667</td>\n",
              "      <td>0.443667</td>\n",
              "      <td>-0.351667</td>\n",
              "      <td>-0.308333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3056</th>\n",
              "      <td>0.207667</td>\n",
              "      <td>-0.133333</td>\n",
              "      <td>-0.381667</td>\n",
              "      <td>0.137333</td>\n",
              "      <td>0.035333</td>\n",
              "      <td>0.178000</td>\n",
              "      <td>0.059000</td>\n",
              "      <td>-0.076000</td>\n",
              "      <td>-0.028000</td>\n",
              "      <td>-0.017333</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.144000</td>\n",
              "      <td>0.055333</td>\n",
              "      <td>0.031000</td>\n",
              "      <td>0.148667</td>\n",
              "      <td>-0.041333</td>\n",
              "      <td>0.270667</td>\n",
              "      <td>0.177667</td>\n",
              "      <td>-0.369333</td>\n",
              "      <td>-0.251667</td>\n",
              "      <td>0.462333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3057</th>\n",
              "      <td>-0.123000</td>\n",
              "      <td>-0.351333</td>\n",
              "      <td>0.032000</td>\n",
              "      <td>0.138000</td>\n",
              "      <td>0.252333</td>\n",
              "      <td>-0.212333</td>\n",
              "      <td>-0.089000</td>\n",
              "      <td>0.194333</td>\n",
              "      <td>-0.080000</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008000</td>\n",
              "      <td>-0.084333</td>\n",
              "      <td>0.063333</td>\n",
              "      <td>-0.074000</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.037667</td>\n",
              "      <td>-0.544333</td>\n",
              "      <td>0.538000</td>\n",
              "      <td>0.079667</td>\n",
              "      <td>-0.391667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3058 rows × 75 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2e01624-bb33-4bfa-8560-329c35c04ba2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d2e01624-bb33-4bfa-8560-329c35c04ba2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d2e01624-bb33-4bfa-8560-329c35c04ba2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       BLOSUM1   BLOSUM2   BLOSUM3   BLOSUM4   BLOSUM5   BLOSUM6   BLOSUM7  \\\n",
              "0     0.154000 -0.646000  0.954000  0.244000  0.392000  0.212000  0.360000   \n",
              "1    -0.194000 -0.660000  0.168000 -0.078000 -0.130000  0.458000  0.328000   \n",
              "2    -0.066000 -0.728000 -0.046000  0.034000  0.056000  0.360000  0.150000   \n",
              "3    -0.806000 -0.624000  0.512000  0.286000  0.254000 -0.734000 -0.128000   \n",
              "4     0.700000 -0.342000  0.958000 -0.074000 -0.380000  0.218000  0.788000   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "3053  0.271667 -0.047667  0.035667 -0.413000  0.118000  0.258333  0.175000   \n",
              "3054  0.187000 -0.166000 -0.031333 -0.120000 -0.024333  0.300000  0.288667   \n",
              "3055 -0.118000 -0.490333  0.300000  0.146000  0.228667 -0.102667  0.052667   \n",
              "3056  0.207667 -0.133333 -0.381667  0.137333  0.035333  0.178000  0.059000   \n",
              "3057 -0.123000 -0.351333  0.032000  0.138000  0.252333 -0.212333 -0.089000   \n",
              "\n",
              "       BLOSUM8   BLOSUM9  BLOSUM10  ...     VHSE4     VHSE5     VHSE6  \\\n",
              "0     0.070000  0.646000  0.134000  ... -0.036000 -0.162000 -0.306000   \n",
              "1    -0.182000  0.242000  0.320000  ... -0.322000 -0.024000 -0.680000   \n",
              "2    -0.284000  0.458000  0.208000  ... -0.730000 -0.034000 -0.334000   \n",
              "3     0.106000  0.004000 -0.236000  ... -0.488000  0.092000 -0.012000   \n",
              "4     0.108000  0.316000  0.076000  ...  0.656000 -0.274000 -0.868000   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "3053  0.073000  0.110000 -0.073333  ...  0.336000  0.305000 -0.265000   \n",
              "3054  0.128000  0.158667  0.000667  ...  0.233667  0.115333 -0.416333   \n",
              "3055 -0.038000  0.137667 -0.020667  ... -0.144000  0.080667 -0.197333   \n",
              "3056 -0.076000 -0.028000 -0.017333  ... -0.144000  0.055333  0.031000   \n",
              "3057  0.194333 -0.080000  0.025000  ... -0.008000 -0.084333  0.063333   \n",
              "\n",
              "         VHSE7     VHSE8        Z1        Z2        Z3        Z4        Z5  \n",
              "0     0.096000 -0.476000  1.152000 -2.288000  1.292000 -0.462000  0.048000  \n",
              "1     0.096000 -0.428000 -0.480000 -2.284000 -0.246000 -0.266000 -0.096000  \n",
              "2    -0.404000 -0.194000 -0.084000 -1.464000  0.366000 -0.366000  0.328000  \n",
              "3    -0.996000  0.590000 -1.358000 -0.754000  1.772000  0.150000 -0.788000  \n",
              "4     0.710000  0.102000  1.014000 -2.248000  0.862000 -0.494000  0.642000  \n",
              "...        ...       ...       ...       ...       ...       ...       ...  \n",
              "3053  0.640000 -0.271333  0.193333 -0.367667 -0.596000  0.286333  0.252000  \n",
              "3054  0.489333 -0.286333  0.058333 -1.056333 -0.479000 -0.249667  0.256667  \n",
              "3055 -0.083333 -0.013333  0.091667 -1.201667  0.443667 -0.351667 -0.308333  \n",
              "3056  0.148667 -0.041333  0.270667  0.177667 -0.369333 -0.251667  0.462333  \n",
              "3057 -0.074000  0.075000  0.037667 -0.544333  0.538000  0.079667 -0.391667  \n",
              "\n",
              "[3058 rows x 75 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "descriptors_data=pd.DataFrame(descr_data)\n",
        "descriptors_data"
      ],
      "id": "f4d7c0f1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "3ea177ab",
        "outputId": "e08739d6-0aee-4447-a9bb-eb17cea95bb7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0f6956fa-9c5f-4445-910a-f452bdd23e5a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>R</th>\n",
              "      <th>N</th>\n",
              "      <th>D</th>\n",
              "      <th>C</th>\n",
              "      <th>E</th>\n",
              "      <th>Q</th>\n",
              "      <th>G</th>\n",
              "      <th>H</th>\n",
              "      <th>I</th>\n",
              "      <th>...</th>\n",
              "      <th>VHSE4</th>\n",
              "      <th>VHSE5</th>\n",
              "      <th>VHSE6</th>\n",
              "      <th>VHSE7</th>\n",
              "      <th>VHSE8</th>\n",
              "      <th>Z1</th>\n",
              "      <th>Z2</th>\n",
              "      <th>Z3</th>\n",
              "      <th>Z4</th>\n",
              "      <th>Z5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.036000</td>\n",
              "      <td>-0.162000</td>\n",
              "      <td>-0.306000</td>\n",
              "      <td>0.096000</td>\n",
              "      <td>-0.476000</td>\n",
              "      <td>1.152000</td>\n",
              "      <td>-2.288000</td>\n",
              "      <td>1.292000</td>\n",
              "      <td>-0.462000</td>\n",
              "      <td>0.048000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.322000</td>\n",
              "      <td>-0.024000</td>\n",
              "      <td>-0.680000</td>\n",
              "      <td>0.096000</td>\n",
              "      <td>-0.428000</td>\n",
              "      <td>-0.480000</td>\n",
              "      <td>-2.284000</td>\n",
              "      <td>-0.246000</td>\n",
              "      <td>-0.266000</td>\n",
              "      <td>-0.096000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.730000</td>\n",
              "      <td>-0.034000</td>\n",
              "      <td>-0.334000</td>\n",
              "      <td>-0.404000</td>\n",
              "      <td>-0.194000</td>\n",
              "      <td>-0.084000</td>\n",
              "      <td>-1.464000</td>\n",
              "      <td>0.366000</td>\n",
              "      <td>-0.366000</td>\n",
              "      <td>0.328000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.488000</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>-0.012000</td>\n",
              "      <td>-0.996000</td>\n",
              "      <td>0.590000</td>\n",
              "      <td>-1.358000</td>\n",
              "      <td>-0.754000</td>\n",
              "      <td>1.772000</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>-0.788000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.656000</td>\n",
              "      <td>-0.274000</td>\n",
              "      <td>-0.868000</td>\n",
              "      <td>0.710000</td>\n",
              "      <td>0.102000</td>\n",
              "      <td>1.014000</td>\n",
              "      <td>-2.248000</td>\n",
              "      <td>0.862000</td>\n",
              "      <td>-0.494000</td>\n",
              "      <td>0.642000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3053</th>\n",
              "      <td>0.13</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.03</td>\n",
              "      <td>...</td>\n",
              "      <td>0.336000</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>-0.265000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>-0.271333</td>\n",
              "      <td>0.193333</td>\n",
              "      <td>-0.367667</td>\n",
              "      <td>-0.596000</td>\n",
              "      <td>0.286333</td>\n",
              "      <td>0.252000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3054</th>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.03</td>\n",
              "      <td>...</td>\n",
              "      <td>0.233667</td>\n",
              "      <td>0.115333</td>\n",
              "      <td>-0.416333</td>\n",
              "      <td>0.489333</td>\n",
              "      <td>-0.286333</td>\n",
              "      <td>0.058333</td>\n",
              "      <td>-1.056333</td>\n",
              "      <td>-0.479000</td>\n",
              "      <td>-0.249667</td>\n",
              "      <td>0.256667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3055</th>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.144000</td>\n",
              "      <td>0.080667</td>\n",
              "      <td>-0.197333</td>\n",
              "      <td>-0.083333</td>\n",
              "      <td>-0.013333</td>\n",
              "      <td>0.091667</td>\n",
              "      <td>-1.201667</td>\n",
              "      <td>0.443667</td>\n",
              "      <td>-0.351667</td>\n",
              "      <td>-0.308333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3056</th>\n",
              "      <td>0.07</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.03</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.144000</td>\n",
              "      <td>0.055333</td>\n",
              "      <td>0.031000</td>\n",
              "      <td>0.148667</td>\n",
              "      <td>-0.041333</td>\n",
              "      <td>0.270667</td>\n",
              "      <td>0.177667</td>\n",
              "      <td>-0.369333</td>\n",
              "      <td>-0.251667</td>\n",
              "      <td>0.462333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3057</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008000</td>\n",
              "      <td>-0.084333</td>\n",
              "      <td>0.063333</td>\n",
              "      <td>-0.074000</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.037667</td>\n",
              "      <td>-0.544333</td>\n",
              "      <td>0.538000</td>\n",
              "      <td>0.079667</td>\n",
              "      <td>-0.391667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3058 rows × 109 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f6956fa-9c5f-4445-910a-f452bdd23e5a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0f6956fa-9c5f-4445-910a-f452bdd23e5a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0f6956fa-9c5f-4445-910a-f452bdd23e5a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         A     R     N     D    C     E     Q     G     H     I  ...  \\\n",
              "0     0.40  0.00  0.00  0.00  0.2  0.00  0.00  0.20  0.00  0.00  ...   \n",
              "1     0.20  0.00  0.00  0.00  0.0  0.00  0.00  0.20  0.00  0.00  ...   \n",
              "2     0.20  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  ...   \n",
              "3     0.00  0.00  0.00  0.00  0.4  0.00  0.00  0.00  0.00  0.00  ...   \n",
              "4     0.20  0.00  0.00  0.00  0.0  0.00  0.00  0.40  0.00  0.00  ...   \n",
              "...    ...   ...   ...   ...  ...   ...   ...   ...   ...   ...  ...   \n",
              "3053  0.13  0.00  0.03  0.03  0.0  0.03  0.03  0.13  0.00  0.03  ...   \n",
              "3054  0.07  0.00  0.00  0.00  0.0  0.03  0.10  0.20  0.03  0.03  ...   \n",
              "3055  0.07  0.00  0.03  0.00  0.2  0.03  0.00  0.10  0.00  0.07  ...   \n",
              "3056  0.07  0.10  0.17  0.03  0.0  0.03  0.07  0.00  0.03  0.03  ...   \n",
              "3057  0.00  0.03  0.00  0.07  0.2  0.07  0.00  0.07  0.07  0.07  ...   \n",
              "\n",
              "         VHSE4     VHSE5     VHSE6     VHSE7     VHSE8        Z1        Z2  \\\n",
              "0    -0.036000 -0.162000 -0.306000  0.096000 -0.476000  1.152000 -2.288000   \n",
              "1    -0.322000 -0.024000 -0.680000  0.096000 -0.428000 -0.480000 -2.284000   \n",
              "2    -0.730000 -0.034000 -0.334000 -0.404000 -0.194000 -0.084000 -1.464000   \n",
              "3    -0.488000  0.092000 -0.012000 -0.996000  0.590000 -1.358000 -0.754000   \n",
              "4     0.656000 -0.274000 -0.868000  0.710000  0.102000  1.014000 -2.248000   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "3053  0.336000  0.305000 -0.265000  0.640000 -0.271333  0.193333 -0.367667   \n",
              "3054  0.233667  0.115333 -0.416333  0.489333 -0.286333  0.058333 -1.056333   \n",
              "3055 -0.144000  0.080667 -0.197333 -0.083333 -0.013333  0.091667 -1.201667   \n",
              "3056 -0.144000  0.055333  0.031000  0.148667 -0.041333  0.270667  0.177667   \n",
              "3057 -0.008000 -0.084333  0.063333 -0.074000  0.075000  0.037667 -0.544333   \n",
              "\n",
              "            Z3        Z4        Z5  \n",
              "0     1.292000 -0.462000  0.048000  \n",
              "1    -0.246000 -0.266000 -0.096000  \n",
              "2     0.366000 -0.366000  0.328000  \n",
              "3     1.772000  0.150000 -0.788000  \n",
              "4     0.862000 -0.494000  0.642000  \n",
              "...        ...       ...       ...  \n",
              "3053 -0.596000  0.286333  0.252000  \n",
              "3054 -0.479000 -0.249667  0.256667  \n",
              "3055  0.443667 -0.351667 -0.308333  \n",
              "3056 -0.369333 -0.251667  0.462333  \n",
              "3057  0.538000  0.079667 -0.391667  \n",
              "\n",
              "[3058 rows x 109 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data=pd.concat([train_data, descriptors_data], axis=1)\n",
        "train_data"
      ],
      "id": "3ea177ab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2b2b4a6c"
      },
      "outputs": [],
      "source": [
        "def load_test(test,amp_result):\n",
        "    test_list=[]\n",
        "    for record in test:\n",
        "        amp_id=record.id\n",
        "        amp_seq=record.seq\n",
        "        peptide=peptides.Peptide(str(amp_seq))\n",
        "        aliph_index=round(peptide.aliphatic_index(),2)\n",
        "        boman=round(peptide.boman(),2)\n",
        "        charge=round(peptide.charge(pKscale=\"Murray\"),2)\n",
        "        amp_size=len(amp_seq)\n",
        "        amp_analysed=ProteinAnalysis(str(amp_seq))\n",
        "        MW=float(\"%0.2f\" % amp_analysed.molecular_weight())\n",
        "        A=round(amp_analysed.get_amino_acids_percent()['A'],2)\n",
        "        R=round(amp_analysed.get_amino_acids_percent()['R'],2)\n",
        "        N=round(amp_analysed.get_amino_acids_percent()['N'],2)\n",
        "        D=round(amp_analysed.get_amino_acids_percent()['D'],2)\n",
        "        C=round(amp_analysed.get_amino_acids_percent()['C'],2)\n",
        "        E=round(amp_analysed.get_amino_acids_percent()['E'],2)\n",
        "        Q=round(amp_analysed.get_amino_acids_percent()['Q'],2)\n",
        "        G=round(amp_analysed.get_amino_acids_percent()['G'],2)\n",
        "        H=round(amp_analysed.get_amino_acids_percent()['H'],2)\n",
        "        I=round(amp_analysed.get_amino_acids_percent()['I'],2)\n",
        "        L=round(amp_analysed.get_amino_acids_percent()['L'],2)\n",
        "        K=round(amp_analysed.get_amino_acids_percent()['K'],2)\n",
        "        M=round(amp_analysed.get_amino_acids_percent()['M'],2)\n",
        "        F=round(amp_analysed.get_amino_acids_percent()['F'],2)\n",
        "        P=round(amp_analysed.get_amino_acids_percent()['P'],2)\n",
        "        S=round(amp_analysed.get_amino_acids_percent()['S'],2)\n",
        "        T=round(amp_analysed.get_amino_acids_percent()['T'],2)\n",
        "        W=round(amp_analysed.get_amino_acids_percent()['W'],2)\n",
        "        Y=round(amp_analysed.get_amino_acids_percent()['Y'],2)\n",
        "        V=round(amp_analysed.get_amino_acids_percent()['V'],2)\n",
        "        IP= float(\"%0.2f\" % amp_analysed.isoelectric_point()) #isoelectric_point\n",
        "        ARM=float(\"%0.2f\" % amp_analysed.aromaticity())#aromaticity\n",
        "        INS=float(\"%0.2f\" % amp_analysed.instability_index()) #instability_index (instability in vitro): < 40==> stable, >= 40 ==> instable \n",
        "        GV=float(\"%0.2f\" % amp_analysed.gravy()) #(grand average of hydropathy) above 0 ==> hydrophobic, beleow 0 ==> hydrophilic\n",
        "        # Secondary Stracture Fractions\n",
        "        amp_SSF=amp_analysed.secondary_structure_fraction()\n",
        "        Helix=float(\"%0.2f\" % amp_SSF[0])\n",
        "        Turn=float(\"%0.2f\" % amp_SSF[1])\n",
        "        Sheet=float(\"%0.2f\" % amp_SSF[2])\n",
        "        #molar extinction coefficient\n",
        "        amp_MEC=amp_analysed.molar_extinction_coefficient()\n",
        "        reduced_cysteines=amp_MEC[0] #reduced_cysteines\n",
        "        cystines_residues=amp_MEC[1] #cystines_residues cys-cys bound\n",
        "        # instability\n",
        "        if float(INS) < 40:\n",
        "            Instability=1 # likely to be stable in essay tube\n",
        "        else:\n",
        "            Instability=0 #likely to be instable in essay tube\n",
        "        # hydropathy\n",
        "        if GV < 0:\n",
        "            Hydrophobicity=0 #hydrophilic\n",
        "        else:\n",
        "            Hydrophobicity=1 #hydrophobic\n",
        "        # AMP Activity\n",
        "        if amp_result=='negative':\n",
        "            amp_activity=0 #negative\n",
        "        else:\n",
        "            amp_activity=1 #positive\n",
        "        amp_phy_ch=(A,R,N,D,C,E,Q,G,H,I,L,K,M,F,P,S,T,W,Y,V,Helix,Turn,Sheet,aliph_index,boman,MW,reduced_cysteines,cystines_residues,IP,charge,ARM,Instability,Hydrophobicity,amp_activity)\n",
        "        test_list.append(amp_phy_ch)\n",
        "    test_df=pd.DataFrame.from_records(test_list,columns=['A','R','N','D','C','E','Q','G','H','I','L','K','M','F','P','S','T','W','Y','V','Helix','Turn','Sheet','aliphatic_index','boman_index','Molecular Weight','MEC RC','MEC SS','Isoelectric Point','Charge','Aromaticity','Instability','Hydrophobicity','AMP Activity'])\n",
        "    return test_df"
      ],
      "id": "2b2b4a6c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4215a293"
      },
      "outputs": [],
      "source": [
        "#load test data\n",
        "test_po_path='test_po.fasta'\n",
        "test_ne_path='test_ne.fasta'\n",
        "test_po_fasta=SeqIO.parse(test_po_path,'fasta')\n",
        "test_ne_fasta=SeqIO.parse(test_ne_path,'fasta')\n",
        "test_po_data=load_test(test_po_fasta,'positive')\n",
        "test_ne_data=load_test(test_ne_fasta,'negative')"
      ],
      "id": "4215a293"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "00d9decd"
      },
      "outputs": [],
      "source": [
        "#concatente test_po data and test_ne data\n",
        "test_data=pd.concat([test_po_data, test_ne_data], axis=0)"
      ],
      "id": "00d9decd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "812c61b6",
        "outputId": "76de7fd5-d3a4-420a-e475-3c37baccff80"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7f4e69ed-8794-42b1-bb05-8c8ac9931068\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>R</th>\n",
              "      <th>N</th>\n",
              "      <th>D</th>\n",
              "      <th>C</th>\n",
              "      <th>E</th>\n",
              "      <th>Q</th>\n",
              "      <th>G</th>\n",
              "      <th>H</th>\n",
              "      <th>I</th>\n",
              "      <th>...</th>\n",
              "      <th>boman_index</th>\n",
              "      <th>Molecular Weight</th>\n",
              "      <th>MEC RC</th>\n",
              "      <th>MEC SS</th>\n",
              "      <th>Isoelectric Point</th>\n",
              "      <th>Charge</th>\n",
              "      <th>Aromaticity</th>\n",
              "      <th>Instability</th>\n",
              "      <th>Hydrophobicity</th>\n",
              "      <th>AMP Activity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.12</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>1.65</td>\n",
              "      <td>2586.81</td>\n",
              "      <td>5500</td>\n",
              "      <td>5500</td>\n",
              "      <td>6.24</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.12</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.01</td>\n",
              "      <td>1615.95</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.59</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>5.58</td>\n",
              "      <td>2814.25</td>\n",
              "      <td>11000</td>\n",
              "      <td>11000</td>\n",
              "      <td>12.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.59</td>\n",
              "      <td>1414.82</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10.48</td>\n",
              "      <td>4.00</td>\n",
              "      <td>0.08</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.09</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.27</td>\n",
              "      <td>2277.87</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10.48</td>\n",
              "      <td>4.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0.07</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>1.82</td>\n",
              "      <td>3351.72</td>\n",
              "      <td>4470</td>\n",
              "      <td>4470</td>\n",
              "      <td>8.42</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.21</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1399.59</td>\n",
              "      <td>1490</td>\n",
              "      <td>1490</td>\n",
              "      <td>9.70</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.15</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>...</td>\n",
              "      <td>1.32</td>\n",
              "      <td>1682.92</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.57</td>\n",
              "      <td>1.09</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>0.10</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.14</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.05</td>\n",
              "      <td>2965.55</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.30</td>\n",
              "      <td>-0.91</td>\n",
              "      <td>0.03</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>1.31</td>\n",
              "      <td>1647.87</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9.73</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>188 rows × 34 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f4e69ed-8794-42b1-bb05-8c8ac9931068')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7f4e69ed-8794-42b1-bb05-8c8ac9931068 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7f4e69ed-8794-42b1-bb05-8c8ac9931068');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       A     R     N     D    C     E     Q     G     H     I  ...  \\\n",
              "0   0.12  0.08  0.00  0.00  0.0  0.08  0.08  0.16  0.00  0.00  ...   \n",
              "1   0.06  0.00  0.00  0.06  0.0  0.00  0.00  0.19  0.00  0.12  ...   \n",
              "2   0.00  0.32  0.05  0.05  0.0  0.00  0.05  0.14  0.00  0.00  ...   \n",
              "3   0.31  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  ...   \n",
              "4   0.17  0.00  0.00  0.00  0.0  0.00  0.00  0.13  0.00  0.09  ...   \n",
              "..   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...  ...   \n",
              "89  0.07  0.04  0.11  0.04  0.0  0.04  0.07  0.00  0.00  0.00  ...   \n",
              "90  0.08  0.00  0.00  0.00  0.0  0.00  0.08  0.15  0.00  0.00  ...   \n",
              "91  0.00  0.00  0.07  0.00  0.0  0.07  0.07  0.07  0.07  0.07  ...   \n",
              "92  0.10  0.03  0.00  0.03  0.0  0.03  0.00  0.14  0.03  0.14  ...   \n",
              "93  0.12  0.00  0.00  0.00  0.0  0.06  0.06  0.12  0.00  0.00  ...   \n",
              "\n",
              "    boman_index  Molecular Weight  MEC RC  MEC SS  Isoelectric Point  Charge  \\\n",
              "0          1.65           2586.81    5500    5500               6.24    0.00   \n",
              "1         -1.01           1615.95       0       0               8.59    1.00   \n",
              "2          5.58           2814.25   11000   11000              12.00    8.00   \n",
              "3         -0.59           1414.82       0       0              10.48    4.00   \n",
              "4         -1.27           2277.87       0       0              10.48    4.00   \n",
              "..          ...               ...     ...     ...                ...     ...   \n",
              "89         1.82           3351.72    4470    4470               8.42    1.00   \n",
              "90         0.75           1399.59    1490    1490               9.70    2.00   \n",
              "91         1.32           1682.92       0       0               8.57    1.09   \n",
              "92        -1.05           2965.55       0       0               5.30   -0.91   \n",
              "93         1.31           1647.87       0       0               9.73    2.00   \n",
              "\n",
              "    Aromaticity  Instability  Hydrophobicity  AMP Activity  \n",
              "0          0.04            1               0             1  \n",
              "1          0.06            1               1             1  \n",
              "2          0.09            0               0             1  \n",
              "3          0.08            1               1             1  \n",
              "4          0.00            1               1             1  \n",
              "..          ...          ...             ...           ...  \n",
              "89         0.21            1               0             0  \n",
              "90         0.15            1               0             0  \n",
              "91         0.07            0               0             0  \n",
              "92         0.03            1               1             0  \n",
              "93         0.06            1               0             0  \n",
              "\n",
              "[188 rows x 34 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data"
      ],
      "id": "812c61b6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97fbac84"
      },
      "outputs": [],
      "source": [
        "def descr_test(test):\n",
        "    test_descp=[]\n",
        "    for record in test:\n",
        "        amp_id=record.id\n",
        "        amp_seq=record.seq\n",
        "        peptide=peptides.Peptide(str(amp_seq))\n",
        "        descriptors=peptide.descriptors() \n",
        "        test_descp.append(descriptors)    \n",
        "    return test_descp"
      ],
      "id": "97fbac84"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "df927e48"
      },
      "outputs": [],
      "source": [
        "test_po_path='test_po.fasta'\n",
        "test_ne_path='test_ne.fasta'\n",
        "test_po_fasta=SeqIO.parse(test_po_path,'fasta')\n",
        "test_ne_fasta=SeqIO.parse(test_ne_path,'fasta')\n",
        "test_po_descr=descr_test(test_po_fasta)\n",
        "test_ne_descr=descr_test(test_ne_fasta)"
      ],
      "id": "df927e48"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "be01d225"
      },
      "outputs": [],
      "source": [
        "po_descr=pd.DataFrame(test_po_descr)\n",
        "ne_descr=pd.DataFrame(test_ne_descr)"
      ],
      "id": "be01d225"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "38ec76ec"
      },
      "outputs": [],
      "source": [
        "desciptors_data=pd.concat([po_descr, ne_descr], axis=0)"
      ],
      "id": "38ec76ec"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "5b2ce149",
        "outputId": "a20d3f5b-01ed-4509-f7ec-44f93c1e34d0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-390498e0-93bd-40e3-96d9-1403a4d5e990\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BLOSUM1</th>\n",
              "      <th>BLOSUM2</th>\n",
              "      <th>BLOSUM3</th>\n",
              "      <th>BLOSUM4</th>\n",
              "      <th>BLOSUM5</th>\n",
              "      <th>BLOSUM6</th>\n",
              "      <th>BLOSUM7</th>\n",
              "      <th>BLOSUM8</th>\n",
              "      <th>BLOSUM9</th>\n",
              "      <th>BLOSUM10</th>\n",
              "      <th>...</th>\n",
              "      <th>VHSE4</th>\n",
              "      <th>VHSE5</th>\n",
              "      <th>VHSE6</th>\n",
              "      <th>VHSE7</th>\n",
              "      <th>VHSE8</th>\n",
              "      <th>Z1</th>\n",
              "      <th>Z2</th>\n",
              "      <th>Z3</th>\n",
              "      <th>Z4</th>\n",
              "      <th>Z5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.302000</td>\n",
              "      <td>-0.240800</td>\n",
              "      <td>0.141600</td>\n",
              "      <td>-0.210000</td>\n",
              "      <td>0.047200</td>\n",
              "      <td>0.310000</td>\n",
              "      <td>0.204800</td>\n",
              "      <td>-0.144800</td>\n",
              "      <td>0.230400</td>\n",
              "      <td>0.110400</td>\n",
              "      <td>...</td>\n",
              "      <td>0.125200</td>\n",
              "      <td>-0.104800</td>\n",
              "      <td>-0.407200</td>\n",
              "      <td>0.318800</td>\n",
              "      <td>-0.167600</td>\n",
              "      <td>0.478400</td>\n",
              "      <td>-1.096800</td>\n",
              "      <td>-0.412000</td>\n",
              "      <td>-0.630800</td>\n",
              "      <td>0.297600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.215625</td>\n",
              "      <td>-0.474375</td>\n",
              "      <td>-0.038750</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>-0.253125</td>\n",
              "      <td>0.407500</td>\n",
              "      <td>0.191875</td>\n",
              "      <td>0.213125</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>-0.008750</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.249375</td>\n",
              "      <td>0.041250</td>\n",
              "      <td>-0.713125</td>\n",
              "      <td>0.426875</td>\n",
              "      <td>-0.335000</td>\n",
              "      <td>-0.686875</td>\n",
              "      <td>-1.556250</td>\n",
              "      <td>-0.636250</td>\n",
              "      <td>-0.537500</td>\n",
              "      <td>0.232500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.444091</td>\n",
              "      <td>0.123182</td>\n",
              "      <td>-0.127727</td>\n",
              "      <td>-0.400000</td>\n",
              "      <td>0.387727</td>\n",
              "      <td>0.365455</td>\n",
              "      <td>0.230909</td>\n",
              "      <td>-0.002273</td>\n",
              "      <td>-0.148636</td>\n",
              "      <td>-0.104545</td>\n",
              "      <td>...</td>\n",
              "      <td>0.674545</td>\n",
              "      <td>0.520455</td>\n",
              "      <td>0.350909</td>\n",
              "      <td>0.651364</td>\n",
              "      <td>-0.038636</td>\n",
              "      <td>1.236818</td>\n",
              "      <td>0.432273</td>\n",
              "      <td>-1.385909</td>\n",
              "      <td>0.628636</td>\n",
              "      <td>-0.128636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.183077</td>\n",
              "      <td>-0.482308</td>\n",
              "      <td>-0.377692</td>\n",
              "      <td>-0.266154</td>\n",
              "      <td>0.293077</td>\n",
              "      <td>0.216154</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.078462</td>\n",
              "      <td>0.371538</td>\n",
              "      <td>-0.265385</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.295385</td>\n",
              "      <td>0.597692</td>\n",
              "      <td>-0.473846</td>\n",
              "      <td>0.534615</td>\n",
              "      <td>-0.313846</td>\n",
              "      <td>-0.863077</td>\n",
              "      <td>-0.690769</td>\n",
              "      <td>-0.958462</td>\n",
              "      <td>0.235385</td>\n",
              "      <td>0.706154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.241304</td>\n",
              "      <td>-0.653913</td>\n",
              "      <td>-0.145217</td>\n",
              "      <td>-0.133043</td>\n",
              "      <td>-0.009565</td>\n",
              "      <td>0.357826</td>\n",
              "      <td>0.296957</td>\n",
              "      <td>0.009565</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.039565</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.395217</td>\n",
              "      <td>0.316957</td>\n",
              "      <td>-0.816957</td>\n",
              "      <td>0.511739</td>\n",
              "      <td>-0.351739</td>\n",
              "      <td>-0.833913</td>\n",
              "      <td>-1.745217</td>\n",
              "      <td>-1.052609</td>\n",
              "      <td>-0.339565</td>\n",
              "      <td>0.398261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0.145000</td>\n",
              "      <td>0.019286</td>\n",
              "      <td>-0.265000</td>\n",
              "      <td>0.038929</td>\n",
              "      <td>-0.083214</td>\n",
              "      <td>-0.033929</td>\n",
              "      <td>0.073929</td>\n",
              "      <td>-0.166786</td>\n",
              "      <td>0.111429</td>\n",
              "      <td>-0.206786</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020000</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>-0.067500</td>\n",
              "      <td>-0.053571</td>\n",
              "      <td>0.168929</td>\n",
              "      <td>-0.244286</td>\n",
              "      <td>0.349286</td>\n",
              "      <td>-0.002143</td>\n",
              "      <td>-0.369643</td>\n",
              "      <td>0.459643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.082308</td>\n",
              "      <td>-0.123846</td>\n",
              "      <td>-0.007692</td>\n",
              "      <td>-0.052308</td>\n",
              "      <td>0.021538</td>\n",
              "      <td>0.330769</td>\n",
              "      <td>0.216154</td>\n",
              "      <td>-0.196154</td>\n",
              "      <td>0.306923</td>\n",
              "      <td>-0.090769</td>\n",
              "      <td>...</td>\n",
              "      <td>0.056154</td>\n",
              "      <td>0.239231</td>\n",
              "      <td>-0.269231</td>\n",
              "      <td>0.213077</td>\n",
              "      <td>-0.291538</td>\n",
              "      <td>0.071538</td>\n",
              "      <td>-1.011538</td>\n",
              "      <td>-0.594615</td>\n",
              "      <td>-0.418462</td>\n",
              "      <td>0.032308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.224667</td>\n",
              "      <td>-0.218667</td>\n",
              "      <td>-0.282667</td>\n",
              "      <td>0.032667</td>\n",
              "      <td>-0.146000</td>\n",
              "      <td>0.081333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>-0.114000</td>\n",
              "      <td>0.096000</td>\n",
              "      <td>0.049333</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.255333</td>\n",
              "      <td>0.097333</td>\n",
              "      <td>-0.271333</td>\n",
              "      <td>0.254000</td>\n",
              "      <td>0.134667</td>\n",
              "      <td>0.174000</td>\n",
              "      <td>-0.576000</td>\n",
              "      <td>-0.551333</td>\n",
              "      <td>-0.298000</td>\n",
              "      <td>0.280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>-0.335172</td>\n",
              "      <td>-0.518966</td>\n",
              "      <td>-0.128276</td>\n",
              "      <td>0.105517</td>\n",
              "      <td>-0.195172</td>\n",
              "      <td>0.353448</td>\n",
              "      <td>0.184138</td>\n",
              "      <td>0.093103</td>\n",
              "      <td>0.121034</td>\n",
              "      <td>0.154828</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.438276</td>\n",
              "      <td>-0.064828</td>\n",
              "      <td>-0.742759</td>\n",
              "      <td>0.235517</td>\n",
              "      <td>-0.301724</td>\n",
              "      <td>-0.926207</td>\n",
              "      <td>-1.522069</td>\n",
              "      <td>-0.548966</td>\n",
              "      <td>-0.532069</td>\n",
              "      <td>0.251724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.379375</td>\n",
              "      <td>-0.320625</td>\n",
              "      <td>0.010625</td>\n",
              "      <td>-0.205000</td>\n",
              "      <td>-0.043125</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>-0.146250</td>\n",
              "      <td>0.366250</td>\n",
              "      <td>-0.051250</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.051250</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>-0.345625</td>\n",
              "      <td>0.382500</td>\n",
              "      <td>0.034375</td>\n",
              "      <td>0.571250</td>\n",
              "      <td>-1.114375</td>\n",
              "      <td>-0.523125</td>\n",
              "      <td>-0.412500</td>\n",
              "      <td>0.274375</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>188 rows × 75 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-390498e0-93bd-40e3-96d9-1403a4d5e990')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-390498e0-93bd-40e3-96d9-1403a4d5e990 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-390498e0-93bd-40e3-96d9-1403a4d5e990');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     BLOSUM1   BLOSUM2   BLOSUM3   BLOSUM4   BLOSUM5   BLOSUM6   BLOSUM7  \\\n",
              "0   0.302000 -0.240800  0.141600 -0.210000  0.047200  0.310000  0.204800   \n",
              "1  -0.215625 -0.474375 -0.038750  0.037500 -0.253125  0.407500  0.191875   \n",
              "2   0.444091  0.123182 -0.127727 -0.400000  0.387727  0.365455  0.230909   \n",
              "3  -0.183077 -0.482308 -0.377692 -0.266154  0.293077  0.216154  0.300000   \n",
              "4  -0.241304 -0.653913 -0.145217 -0.133043 -0.009565  0.357826  0.296957   \n",
              "..       ...       ...       ...       ...       ...       ...       ...   \n",
              "89  0.145000  0.019286 -0.265000  0.038929 -0.083214 -0.033929  0.073929   \n",
              "90  0.082308 -0.123846 -0.007692 -0.052308  0.021538  0.330769  0.216154   \n",
              "91  0.224667 -0.218667 -0.282667  0.032667 -0.146000  0.081333  0.083333   \n",
              "92 -0.335172 -0.518966 -0.128276  0.105517 -0.195172  0.353448  0.184138   \n",
              "93  0.379375 -0.320625  0.010625 -0.205000 -0.043125  0.174375  0.198750   \n",
              "\n",
              "     BLOSUM8   BLOSUM9  BLOSUM10  ...     VHSE4     VHSE5     VHSE6     VHSE7  \\\n",
              "0  -0.144800  0.230400  0.110400  ...  0.125200 -0.104800 -0.407200  0.318800   \n",
              "1   0.213125  0.130000 -0.008750  ... -0.249375  0.041250 -0.713125  0.426875   \n",
              "2  -0.002273 -0.148636 -0.104545  ...  0.674545  0.520455  0.350909  0.651364   \n",
              "3   0.078462  0.371538 -0.265385  ... -0.295385  0.597692 -0.473846  0.534615   \n",
              "4   0.009565  0.200000  0.039565  ... -0.395217  0.316957 -0.816957  0.511739   \n",
              "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
              "89 -0.166786  0.111429 -0.206786  ... -0.020000  0.037500 -0.067500 -0.053571   \n",
              "90 -0.196154  0.306923 -0.090769  ...  0.056154  0.239231 -0.269231  0.213077   \n",
              "91 -0.114000  0.096000  0.049333  ... -0.255333  0.097333 -0.271333  0.254000   \n",
              "92  0.093103  0.121034  0.154828  ... -0.438276 -0.064828 -0.742759  0.235517   \n",
              "93 -0.146250  0.366250 -0.051250  ... -0.051250  0.129375 -0.345625  0.382500   \n",
              "\n",
              "       VHSE8        Z1        Z2        Z3        Z4        Z5  \n",
              "0  -0.167600  0.478400 -1.096800 -0.412000 -0.630800  0.297600  \n",
              "1  -0.335000 -0.686875 -1.556250 -0.636250 -0.537500  0.232500  \n",
              "2  -0.038636  1.236818  0.432273 -1.385909  0.628636 -0.128636  \n",
              "3  -0.313846 -0.863077 -0.690769 -0.958462  0.235385  0.706154  \n",
              "4  -0.351739 -0.833913 -1.745217 -1.052609 -0.339565  0.398261  \n",
              "..       ...       ...       ...       ...       ...       ...  \n",
              "89  0.168929 -0.244286  0.349286 -0.002143 -0.369643  0.459643  \n",
              "90 -0.291538  0.071538 -1.011538 -0.594615 -0.418462  0.032308  \n",
              "91  0.134667  0.174000 -0.576000 -0.551333 -0.298000  0.280000  \n",
              "92 -0.301724 -0.926207 -1.522069 -0.548966 -0.532069  0.251724  \n",
              "93  0.034375  0.571250 -1.114375 -0.523125 -0.412500  0.274375  \n",
              "\n",
              "[188 rows x 75 columns]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "desciptors_data"
      ],
      "id": "5b2ce149"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73e952ff"
      },
      "outputs": [],
      "source": [
        "test_data=pd.concat([test_data, desciptors_data], axis=1)"
      ],
      "id": "73e952ff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "93fd0eb5",
        "outputId": "f220b7ef-949f-4bf1-ee98-d4c316873902"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d037541c-6d8a-44fa-8ddc-072d74d279f6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>R</th>\n",
              "      <th>N</th>\n",
              "      <th>D</th>\n",
              "      <th>C</th>\n",
              "      <th>E</th>\n",
              "      <th>Q</th>\n",
              "      <th>G</th>\n",
              "      <th>H</th>\n",
              "      <th>I</th>\n",
              "      <th>...</th>\n",
              "      <th>VHSE4</th>\n",
              "      <th>VHSE5</th>\n",
              "      <th>VHSE6</th>\n",
              "      <th>VHSE7</th>\n",
              "      <th>VHSE8</th>\n",
              "      <th>Z1</th>\n",
              "      <th>Z2</th>\n",
              "      <th>Z3</th>\n",
              "      <th>Z4</th>\n",
              "      <th>Z5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.12</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.125200</td>\n",
              "      <td>-0.104800</td>\n",
              "      <td>-0.407200</td>\n",
              "      <td>0.318800</td>\n",
              "      <td>-0.167600</td>\n",
              "      <td>0.478400</td>\n",
              "      <td>-1.096800</td>\n",
              "      <td>-0.412000</td>\n",
              "      <td>-0.630800</td>\n",
              "      <td>0.297600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.12</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.249375</td>\n",
              "      <td>0.041250</td>\n",
              "      <td>-0.713125</td>\n",
              "      <td>0.426875</td>\n",
              "      <td>-0.335000</td>\n",
              "      <td>-0.686875</td>\n",
              "      <td>-1.556250</td>\n",
              "      <td>-0.636250</td>\n",
              "      <td>-0.537500</td>\n",
              "      <td>0.232500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.674545</td>\n",
              "      <td>0.520455</td>\n",
              "      <td>0.350909</td>\n",
              "      <td>0.651364</td>\n",
              "      <td>-0.038636</td>\n",
              "      <td>1.236818</td>\n",
              "      <td>0.432273</td>\n",
              "      <td>-1.385909</td>\n",
              "      <td>0.628636</td>\n",
              "      <td>-0.128636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.295385</td>\n",
              "      <td>0.597692</td>\n",
              "      <td>-0.473846</td>\n",
              "      <td>0.534615</td>\n",
              "      <td>-0.313846</td>\n",
              "      <td>-0.863077</td>\n",
              "      <td>-0.690769</td>\n",
              "      <td>-0.958462</td>\n",
              "      <td>0.235385</td>\n",
              "      <td>0.706154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.09</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.395217</td>\n",
              "      <td>0.316957</td>\n",
              "      <td>-0.816957</td>\n",
              "      <td>0.511739</td>\n",
              "      <td>-0.351739</td>\n",
              "      <td>-0.833913</td>\n",
              "      <td>-1.745217</td>\n",
              "      <td>-1.052609</td>\n",
              "      <td>-0.339565</td>\n",
              "      <td>0.398261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0.07</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020000</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>-0.067500</td>\n",
              "      <td>-0.053571</td>\n",
              "      <td>0.168929</td>\n",
              "      <td>-0.244286</td>\n",
              "      <td>0.349286</td>\n",
              "      <td>-0.002143</td>\n",
              "      <td>-0.369643</td>\n",
              "      <td>0.459643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.056154</td>\n",
              "      <td>0.239231</td>\n",
              "      <td>-0.269231</td>\n",
              "      <td>0.213077</td>\n",
              "      <td>-0.291538</td>\n",
              "      <td>0.071538</td>\n",
              "      <td>-1.011538</td>\n",
              "      <td>-0.594615</td>\n",
              "      <td>-0.418462</td>\n",
              "      <td>0.032308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.255333</td>\n",
              "      <td>0.097333</td>\n",
              "      <td>-0.271333</td>\n",
              "      <td>0.254000</td>\n",
              "      <td>0.134667</td>\n",
              "      <td>0.174000</td>\n",
              "      <td>-0.576000</td>\n",
              "      <td>-0.551333</td>\n",
              "      <td>-0.298000</td>\n",
              "      <td>0.280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>0.10</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.14</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.438276</td>\n",
              "      <td>-0.064828</td>\n",
              "      <td>-0.742759</td>\n",
              "      <td>0.235517</td>\n",
              "      <td>-0.301724</td>\n",
              "      <td>-0.926207</td>\n",
              "      <td>-1.522069</td>\n",
              "      <td>-0.548966</td>\n",
              "      <td>-0.532069</td>\n",
              "      <td>0.251724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.051250</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>-0.345625</td>\n",
              "      <td>0.382500</td>\n",
              "      <td>0.034375</td>\n",
              "      <td>0.571250</td>\n",
              "      <td>-1.114375</td>\n",
              "      <td>-0.523125</td>\n",
              "      <td>-0.412500</td>\n",
              "      <td>0.274375</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>188 rows × 109 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d037541c-6d8a-44fa-8ddc-072d74d279f6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d037541c-6d8a-44fa-8ddc-072d74d279f6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d037541c-6d8a-44fa-8ddc-072d74d279f6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       A     R     N     D    C     E     Q     G     H     I  ...     VHSE4  \\\n",
              "0   0.12  0.08  0.00  0.00  0.0  0.08  0.08  0.16  0.00  0.00  ...  0.125200   \n",
              "1   0.06  0.00  0.00  0.06  0.0  0.00  0.00  0.19  0.00  0.12  ... -0.249375   \n",
              "2   0.00  0.32  0.05  0.05  0.0  0.00  0.05  0.14  0.00  0.00  ...  0.674545   \n",
              "3   0.31  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  ... -0.295385   \n",
              "4   0.17  0.00  0.00  0.00  0.0  0.00  0.00  0.13  0.00  0.09  ... -0.395217   \n",
              "..   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...  ...       ...   \n",
              "89  0.07  0.04  0.11  0.04  0.0  0.04  0.07  0.00  0.00  0.00  ... -0.020000   \n",
              "90  0.08  0.00  0.00  0.00  0.0  0.00  0.08  0.15  0.00  0.00  ...  0.056154   \n",
              "91  0.00  0.00  0.07  0.00  0.0  0.07  0.07  0.07  0.07  0.07  ... -0.255333   \n",
              "92  0.10  0.03  0.00  0.03  0.0  0.03  0.00  0.14  0.03  0.14  ... -0.438276   \n",
              "93  0.12  0.00  0.00  0.00  0.0  0.06  0.06  0.12  0.00  0.00  ... -0.051250   \n",
              "\n",
              "       VHSE5     VHSE6     VHSE7     VHSE8        Z1        Z2        Z3  \\\n",
              "0  -0.104800 -0.407200  0.318800 -0.167600  0.478400 -1.096800 -0.412000   \n",
              "1   0.041250 -0.713125  0.426875 -0.335000 -0.686875 -1.556250 -0.636250   \n",
              "2   0.520455  0.350909  0.651364 -0.038636  1.236818  0.432273 -1.385909   \n",
              "3   0.597692 -0.473846  0.534615 -0.313846 -0.863077 -0.690769 -0.958462   \n",
              "4   0.316957 -0.816957  0.511739 -0.351739 -0.833913 -1.745217 -1.052609   \n",
              "..       ...       ...       ...       ...       ...       ...       ...   \n",
              "89  0.037500 -0.067500 -0.053571  0.168929 -0.244286  0.349286 -0.002143   \n",
              "90  0.239231 -0.269231  0.213077 -0.291538  0.071538 -1.011538 -0.594615   \n",
              "91  0.097333 -0.271333  0.254000  0.134667  0.174000 -0.576000 -0.551333   \n",
              "92 -0.064828 -0.742759  0.235517 -0.301724 -0.926207 -1.522069 -0.548966   \n",
              "93  0.129375 -0.345625  0.382500  0.034375  0.571250 -1.114375 -0.523125   \n",
              "\n",
              "          Z4        Z5  \n",
              "0  -0.630800  0.297600  \n",
              "1  -0.537500  0.232500  \n",
              "2   0.628636 -0.128636  \n",
              "3   0.235385  0.706154  \n",
              "4  -0.339565  0.398261  \n",
              "..       ...       ...  \n",
              "89 -0.369643  0.459643  \n",
              "90 -0.418462  0.032308  \n",
              "91 -0.298000  0.280000  \n",
              "92 -0.532069  0.251724  \n",
              "93 -0.412500  0.274375  \n",
              "\n",
              "[188 rows x 109 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data"
      ],
      "id": "93fd0eb5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fe42297"
      },
      "outputs": [],
      "source": [
        "# X_train, Y_train\n",
        "X_train = train_data.drop('AMP Activity', axis=1)\n",
        "Y_train = train_data['AMP Activity'].copy()\n",
        "# convert to numpy\n",
        "XTRAIN=X_train.to_numpy()\n",
        "YTRAIN=Y_train.to_numpy()"
      ],
      "id": "4fe42297"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b40abd1"
      },
      "outputs": [],
      "source": [
        "# X_test, Y_test\n",
        "X_test = test_data.drop('AMP Activity', axis=1)\n",
        "Y_test = test_data['AMP Activity'].copy()\n",
        "# convert to numpy\n",
        "XTEST=X_test.to_numpy()\n",
        "YTEST=Y_test.to_numpy()"
      ],
      "id": "6b40abd1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fde87d1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout"
      ],
      "id": "4fde87d1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ca1861c",
        "outputId": "b025eb5c-7b95-4503-ef67-2a5b5dee2571"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3058, 108)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "XTRAIN.shape"
      ],
      "id": "8ca1861c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "327b4518"
      },
      "outputs": [],
      "source": [
        "model=Sequential()\n",
        "model.add(Dense((54),input_dim=108,activation='relu'))\n",
        "model.add(Dense(54, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "id": "327b4518"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe380900"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "id": "fe380900"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf8868c4"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "callback1=ModelCheckpoint(filepath='best_amp.hdf5',monitor='val_loss',save_best_only=True,save_weights=True)\n",
        "callback2=EarlyStopping(monitor='val_loss',mode='min',patience=100,verbose=1)"
      ],
      "id": "bf8868c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3487115c",
        "outputId": "691a86cf-0be1-40dd-f59a-243204700922"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8000\n",
            "26/26 [==============================] - 1s 13ms/step - loss: 26.3917 - accuracy: 0.5198 - val_loss: 45.2076 - val_accuracy: 0.4947\n",
            "Epoch 2/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 29.6745 - accuracy: 0.5256 - val_loss: 5.2539 - val_accuracy: 0.5160\n",
            "Epoch 3/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 8.6076 - accuracy: 0.5455 - val_loss: 9.7619 - val_accuracy: 0.6064\n",
            "Epoch 4/8000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 19.0480 - accuracy: 0.5451 - val_loss: 18.9774 - val_accuracy: 0.6596\n",
            "Epoch 5/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 12.2921 - accuracy: 0.5535 - val_loss: 11.2044 - val_accuracy: 0.5479\n",
            "Epoch 6/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 7.4935 - accuracy: 0.5658 - val_loss: 6.4282 - val_accuracy: 0.5532\n",
            "Epoch 7/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 8.9805 - accuracy: 0.5701 - val_loss: 4.7029 - val_accuracy: 0.5691\n",
            "Epoch 8/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 10.1469 - accuracy: 0.5589 - val_loss: 10.8063 - val_accuracy: 0.6436\n",
            "Epoch 9/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 7.0794 - accuracy: 0.5904 - val_loss: 2.3442 - val_accuracy: 0.6702\n",
            "Epoch 10/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 5.4932 - accuracy: 0.5977 - val_loss: 21.4351 - val_accuracy: 0.5000\n",
            "Epoch 11/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 9.2258 - accuracy: 0.5596 - val_loss: 6.0726 - val_accuracy: 0.5479\n",
            "Epoch 12/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 3.9119 - accuracy: 0.6038 - val_loss: 1.6041 - val_accuracy: 0.7394\n",
            "Epoch 13/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 5.6567 - accuracy: 0.6064 - val_loss: 9.3899 - val_accuracy: 0.5479\n",
            "Epoch 14/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 6.5820 - accuracy: 0.5683 - val_loss: 4.4476 - val_accuracy: 0.6330\n",
            "Epoch 15/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 4.2411 - accuracy: 0.6020 - val_loss: 4.5504 - val_accuracy: 0.5904\n",
            "Epoch 16/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 9.4609 - accuracy: 0.5524 - val_loss: 14.5537 - val_accuracy: 0.6064\n",
            "Epoch 17/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 12.5451 - accuracy: 0.5723 - val_loss: 3.6481 - val_accuracy: 0.6543\n",
            "Epoch 18/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 5.7403 - accuracy: 0.5922 - val_loss: 1.9600 - val_accuracy: 0.7021\n",
            "Epoch 19/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.9629 - accuracy: 0.6397 - val_loss: 6.1275 - val_accuracy: 0.6011\n",
            "Epoch 20/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 3.6849 - accuracy: 0.5810 - val_loss: 5.2081 - val_accuracy: 0.6915\n",
            "Epoch 21/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 5.1767 - accuracy: 0.5788 - val_loss: 1.5951 - val_accuracy: 0.7074\n",
            "Epoch 22/8000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 7.2540 - accuracy: 0.5676 - val_loss: 3.7991 - val_accuracy: 0.6117\n",
            "Epoch 23/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 3.9198 - accuracy: 0.6191 - val_loss: 3.5759 - val_accuracy: 0.6489\n",
            "Epoch 24/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 3.4897 - accuracy: 0.6151 - val_loss: 1.2647 - val_accuracy: 0.7713\n",
            "Epoch 25/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.7797 - accuracy: 0.6404 - val_loss: 2.2436 - val_accuracy: 0.6755\n",
            "Epoch 26/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 4.7006 - accuracy: 0.5973 - val_loss: 5.2356 - val_accuracy: 0.6649\n",
            "Epoch 27/8000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 4.4224 - accuracy: 0.5944 - val_loss: 1.4681 - val_accuracy: 0.7447\n",
            "Epoch 28/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 5.9474 - accuracy: 0.5941 - val_loss: 7.1292 - val_accuracy: 0.5691\n",
            "Epoch 29/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 5.4046 - accuracy: 0.5879 - val_loss: 3.1712 - val_accuracy: 0.5957\n",
            "Epoch 30/8000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 4.5867 - accuracy: 0.6100 - val_loss: 3.2966 - val_accuracy: 0.6755\n",
            "Epoch 31/8000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 4.9277 - accuracy: 0.5919 - val_loss: 9.2416 - val_accuracy: 0.6011\n",
            "Epoch 32/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 5.1701 - accuracy: 0.6020 - val_loss: 1.7351 - val_accuracy: 0.7234\n",
            "Epoch 33/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 4.6645 - accuracy: 0.5991 - val_loss: 10.3523 - val_accuracy: 0.5160\n",
            "Epoch 34/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 9.0022 - accuracy: 0.5777 - val_loss: 3.3230 - val_accuracy: 0.6064\n",
            "Epoch 35/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 6.5459 - accuracy: 0.5951 - val_loss: 3.8455 - val_accuracy: 0.7234\n",
            "Epoch 36/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 3.2934 - accuracy: 0.6347 - val_loss: 4.5536 - val_accuracy: 0.5957\n",
            "Epoch 37/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 3.4355 - accuracy: 0.6256 - val_loss: 2.4461 - val_accuracy: 0.6862\n",
            "Epoch 38/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 3.5318 - accuracy: 0.6133 - val_loss: 3.9304 - val_accuracy: 0.7181\n",
            "Epoch 39/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.8629 - accuracy: 0.6375 - val_loss: 2.9442 - val_accuracy: 0.6915\n",
            "Epoch 40/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 3.3331 - accuracy: 0.6154 - val_loss: 3.7385 - val_accuracy: 0.6543\n",
            "Epoch 41/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.6819 - accuracy: 0.6216 - val_loss: 2.1847 - val_accuracy: 0.6543\n",
            "Epoch 42/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 5.1670 - accuracy: 0.6017 - val_loss: 2.9577 - val_accuracy: 0.6543\n",
            "Epoch 43/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 3.6346 - accuracy: 0.6028 - val_loss: 5.0396 - val_accuracy: 0.6862\n",
            "Epoch 44/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.1854 - accuracy: 0.6600 - val_loss: 1.3729 - val_accuracy: 0.7553\n",
            "Epoch 45/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.7473 - accuracy: 0.6260 - val_loss: 2.1556 - val_accuracy: 0.7447\n",
            "Epoch 46/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 4.1006 - accuracy: 0.5893 - val_loss: 2.3286 - val_accuracy: 0.7021\n",
            "Epoch 47/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.6898 - accuracy: 0.6133 - val_loss: 1.8207 - val_accuracy: 0.7021\n",
            "Epoch 48/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.0810 - accuracy: 0.6448 - val_loss: 3.8068 - val_accuracy: 0.6170\n",
            "Epoch 49/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.4480 - accuracy: 0.6386 - val_loss: 4.2087 - val_accuracy: 0.5426\n",
            "Epoch 50/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 4.0529 - accuracy: 0.5868 - val_loss: 8.0606 - val_accuracy: 0.6543\n",
            "Epoch 51/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 3.1390 - accuracy: 0.6053 - val_loss: 1.5377 - val_accuracy: 0.7181\n",
            "Epoch 52/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.3304 - accuracy: 0.6357 - val_loss: 3.0968 - val_accuracy: 0.6915\n",
            "Epoch 53/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.6586 - accuracy: 0.6267 - val_loss: 2.6230 - val_accuracy: 0.6011\n",
            "Epoch 54/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 3.3365 - accuracy: 0.5919 - val_loss: 1.7622 - val_accuracy: 0.7128\n",
            "Epoch 55/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.9111 - accuracy: 0.6423 - val_loss: 1.1616 - val_accuracy: 0.7394\n",
            "Epoch 56/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.5662 - accuracy: 0.6517 - val_loss: 1.8245 - val_accuracy: 0.6862\n",
            "Epoch 57/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.8787 - accuracy: 0.6386 - val_loss: 1.3797 - val_accuracy: 0.6649\n",
            "Epoch 58/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.9611 - accuracy: 0.6452 - val_loss: 4.1146 - val_accuracy: 0.6011\n",
            "Epoch 59/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 3.0126 - accuracy: 0.6147 - val_loss: 1.4257 - val_accuracy: 0.6862\n",
            "Epoch 60/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.2564 - accuracy: 0.6506 - val_loss: 2.7472 - val_accuracy: 0.6489\n",
            "Epoch 61/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 6.5403 - accuracy: 0.5462 - val_loss: 4.4443 - val_accuracy: 0.5798\n",
            "Epoch 62/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 3.0437 - accuracy: 0.6241 - val_loss: 2.8394 - val_accuracy: 0.6968\n",
            "Epoch 63/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.8470 - accuracy: 0.6506 - val_loss: 0.9923 - val_accuracy: 0.7340\n",
            "Epoch 64/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.8200 - accuracy: 0.6252 - val_loss: 4.9132 - val_accuracy: 0.6117\n",
            "Epoch 65/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.0317 - accuracy: 0.6415 - val_loss: 1.6291 - val_accuracy: 0.7340\n",
            "Epoch 66/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.4078 - accuracy: 0.6241 - val_loss: 2.0543 - val_accuracy: 0.6596\n",
            "Epoch 67/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.2156 - accuracy: 0.6810 - val_loss: 1.1922 - val_accuracy: 0.7447\n",
            "Epoch 68/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.5438 - accuracy: 0.6539 - val_loss: 3.1977 - val_accuracy: 0.5691\n",
            "Epoch 69/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.5810 - accuracy: 0.6107 - val_loss: 1.9701 - val_accuracy: 0.6862\n",
            "Epoch 70/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.0259 - accuracy: 0.6477 - val_loss: 3.7659 - val_accuracy: 0.6277\n",
            "Epoch 71/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 3.6907 - accuracy: 0.5879 - val_loss: 2.3028 - val_accuracy: 0.6064\n",
            "Epoch 72/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.4012 - accuracy: 0.6734 - val_loss: 1.4913 - val_accuracy: 0.7340\n",
            "Epoch 73/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.6709 - accuracy: 0.6332 - val_loss: 2.4095 - val_accuracy: 0.6330\n",
            "Epoch 74/8000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 1.5336 - accuracy: 0.6531 - val_loss: 1.6334 - val_accuracy: 0.7340\n",
            "Epoch 75/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 3.2557 - accuracy: 0.5973 - val_loss: 3.7280 - val_accuracy: 0.6223\n",
            "Epoch 76/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.1158 - accuracy: 0.6499 - val_loss: 3.4899 - val_accuracy: 0.6755\n",
            "Epoch 77/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 2.1335 - accuracy: 0.6318 - val_loss: 2.1093 - val_accuracy: 0.6596\n",
            "Epoch 78/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.7078 - accuracy: 0.6433 - val_loss: 4.2923 - val_accuracy: 0.5479\n",
            "Epoch 79/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 4.7770 - accuracy: 0.5469 - val_loss: 6.3093 - val_accuracy: 0.5213\n",
            "Epoch 80/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 4.9760 - accuracy: 0.5647 - val_loss: 1.9701 - val_accuracy: 0.7287\n",
            "Epoch 81/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.6099 - accuracy: 0.6836 - val_loss: 1.2080 - val_accuracy: 0.7287\n",
            "Epoch 82/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.1951 - accuracy: 0.6814 - val_loss: 1.3766 - val_accuracy: 0.7500\n",
            "Epoch 83/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.4349 - accuracy: 0.6477 - val_loss: 1.1066 - val_accuracy: 0.6968\n",
            "Epoch 84/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.4621 - accuracy: 0.6553 - val_loss: 2.8389 - val_accuracy: 0.5585\n",
            "Epoch 85/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.6840 - accuracy: 0.5897 - val_loss: 2.0086 - val_accuracy: 0.6649\n",
            "Epoch 86/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.6824 - accuracy: 0.6169 - val_loss: 1.7811 - val_accuracy: 0.6968\n",
            "Epoch 87/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.8120 - accuracy: 0.6506 - val_loss: 2.3394 - val_accuracy: 0.6862\n",
            "Epoch 88/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.7042 - accuracy: 0.6459 - val_loss: 1.3255 - val_accuracy: 0.6968\n",
            "Epoch 89/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.4593 - accuracy: 0.6676 - val_loss: 1.1587 - val_accuracy: 0.7553\n",
            "Epoch 90/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.7808 - accuracy: 0.6444 - val_loss: 1.6843 - val_accuracy: 0.7074\n",
            "Epoch 91/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.1360 - accuracy: 0.6285 - val_loss: 1.2598 - val_accuracy: 0.7234\n",
            "Epoch 92/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.2842 - accuracy: 0.6289 - val_loss: 0.9258 - val_accuracy: 0.7287\n",
            "Epoch 93/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.8119 - accuracy: 0.6404 - val_loss: 1.0046 - val_accuracy: 0.7074\n",
            "Epoch 94/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.0954 - accuracy: 0.6861 - val_loss: 0.9999 - val_accuracy: 0.7128\n",
            "Epoch 95/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.0360 - accuracy: 0.6865 - val_loss: 0.8934 - val_accuracy: 0.7500\n",
            "Epoch 96/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.1814 - accuracy: 0.6082 - val_loss: 4.0722 - val_accuracy: 0.5160\n",
            "Epoch 97/8000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 1.9295 - accuracy: 0.6260 - val_loss: 4.1951 - val_accuracy: 0.5479\n",
            "Epoch 98/8000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 3.9207 - accuracy: 0.5792 - val_loss: 2.6683 - val_accuracy: 0.7128\n",
            "Epoch 99/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.9133 - accuracy: 0.6354 - val_loss: 2.8659 - val_accuracy: 0.6915\n",
            "Epoch 100/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.8787 - accuracy: 0.6231 - val_loss: 1.4080 - val_accuracy: 0.7447\n",
            "Epoch 101/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.3859 - accuracy: 0.6597 - val_loss: 1.4860 - val_accuracy: 0.7394\n",
            "Epoch 102/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 2.3333 - accuracy: 0.6046 - val_loss: 0.8217 - val_accuracy: 0.7181\n",
            "Epoch 103/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 3.2143 - accuracy: 0.6031 - val_loss: 5.4031 - val_accuracy: 0.6702\n",
            "Epoch 104/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.3885 - accuracy: 0.6086 - val_loss: 1.7506 - val_accuracy: 0.6011\n",
            "Epoch 105/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.7144 - accuracy: 0.6510 - val_loss: 0.8426 - val_accuracy: 0.7660\n",
            "Epoch 106/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.1101 - accuracy: 0.6212 - val_loss: 2.7021 - val_accuracy: 0.6170\n",
            "Epoch 107/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.4936 - accuracy: 0.6462 - val_loss: 1.2806 - val_accuracy: 0.7234\n",
            "Epoch 108/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.1351 - accuracy: 0.6343 - val_loss: 3.0300 - val_accuracy: 0.6968\n",
            "Epoch 109/8000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 2.0506 - accuracy: 0.6209 - val_loss: 3.1480 - val_accuracy: 0.6968\n",
            "Epoch 110/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.2824 - accuracy: 0.6785 - val_loss: 0.9877 - val_accuracy: 0.7128\n",
            "Epoch 111/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.9237 - accuracy: 0.6894 - val_loss: 0.7133 - val_accuracy: 0.7287\n",
            "Epoch 112/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.1762 - accuracy: 0.6694 - val_loss: 1.7004 - val_accuracy: 0.6649\n",
            "Epoch 113/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.6431 - accuracy: 0.6202 - val_loss: 1.0736 - val_accuracy: 0.7447\n",
            "Epoch 114/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.0931 - accuracy: 0.6876 - val_loss: 1.3456 - val_accuracy: 0.6702\n",
            "Epoch 115/8000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.9566 - accuracy: 0.6966 - val_loss: 1.3789 - val_accuracy: 0.6489\n",
            "Epoch 116/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.0444 - accuracy: 0.6839 - val_loss: 1.1717 - val_accuracy: 0.6702\n",
            "Epoch 117/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.0072 - accuracy: 0.6694 - val_loss: 0.7657 - val_accuracy: 0.7606\n",
            "Epoch 118/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.7262 - accuracy: 0.6296 - val_loss: 1.6833 - val_accuracy: 0.7074\n",
            "Epoch 119/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.6021 - accuracy: 0.6310 - val_loss: 2.7392 - val_accuracy: 0.6755\n",
            "Epoch 120/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.7784 - accuracy: 0.5912 - val_loss: 2.7894 - val_accuracy: 0.7021\n",
            "Epoch 121/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.8011 - accuracy: 0.5962 - val_loss: 3.1305 - val_accuracy: 0.5851\n",
            "Epoch 122/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.6309 - accuracy: 0.6618 - val_loss: 1.0466 - val_accuracy: 0.7234\n",
            "Epoch 123/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.1897 - accuracy: 0.6597 - val_loss: 1.6425 - val_accuracy: 0.6915\n",
            "Epoch 124/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.4470 - accuracy: 0.6267 - val_loss: 0.8641 - val_accuracy: 0.7447\n",
            "Epoch 125/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.7169 - accuracy: 0.6499 - val_loss: 1.4026 - val_accuracy: 0.6596\n",
            "Epoch 126/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.3433 - accuracy: 0.6549 - val_loss: 2.1181 - val_accuracy: 0.5798\n",
            "Epoch 127/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.4592 - accuracy: 0.6020 - val_loss: 0.9702 - val_accuracy: 0.6915\n",
            "Epoch 128/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.0644 - accuracy: 0.6952 - val_loss: 1.2901 - val_accuracy: 0.6702\n",
            "Epoch 129/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.1209 - accuracy: 0.6506 - val_loss: 1.8400 - val_accuracy: 0.6702\n",
            "Epoch 130/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.6452 - accuracy: 0.6183 - val_loss: 1.7471 - val_accuracy: 0.7287\n",
            "Epoch 131/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.5338 - accuracy: 0.6466 - val_loss: 1.1248 - val_accuracy: 0.6968\n",
            "Epoch 132/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8639 - accuracy: 0.7046 - val_loss: 1.0076 - val_accuracy: 0.6915\n",
            "Epoch 133/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.0414 - accuracy: 0.6803 - val_loss: 0.7441 - val_accuracy: 0.7872\n",
            "Epoch 134/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.0022 - accuracy: 0.6125 - val_loss: 2.1314 - val_accuracy: 0.6117\n",
            "Epoch 135/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.4531 - accuracy: 0.6071 - val_loss: 2.2262 - val_accuracy: 0.6862\n",
            "Epoch 136/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.0996 - accuracy: 0.6365 - val_loss: 1.0945 - val_accuracy: 0.7234\n",
            "Epoch 137/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.9460 - accuracy: 0.6894 - val_loss: 0.7706 - val_accuracy: 0.7340\n",
            "Epoch 138/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8110 - accuracy: 0.6988 - val_loss: 0.7711 - val_accuracy: 0.7287\n",
            "Epoch 139/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.2373 - accuracy: 0.6611 - val_loss: 0.8943 - val_accuracy: 0.7021\n",
            "Epoch 140/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.9811 - accuracy: 0.6673 - val_loss: 1.8320 - val_accuracy: 0.6223\n",
            "Epoch 141/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.4125 - accuracy: 0.6350 - val_loss: 1.2784 - val_accuracy: 0.6011\n",
            "Epoch 142/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.1891 - accuracy: 0.6673 - val_loss: 0.8598 - val_accuracy: 0.7660\n",
            "Epoch 143/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8928 - accuracy: 0.6912 - val_loss: 0.8002 - val_accuracy: 0.7394\n",
            "Epoch 144/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8708 - accuracy: 0.6847 - val_loss: 1.1719 - val_accuracy: 0.7340\n",
            "Epoch 145/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.2801 - accuracy: 0.6307 - val_loss: 1.7217 - val_accuracy: 0.6649\n",
            "Epoch 146/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.1065 - accuracy: 0.6600 - val_loss: 0.7230 - val_accuracy: 0.7447\n",
            "Epoch 147/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.9642 - accuracy: 0.6789 - val_loss: 0.7050 - val_accuracy: 0.7287\n",
            "Epoch 148/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.1166 - accuracy: 0.6589 - val_loss: 1.5908 - val_accuracy: 0.7234\n",
            "Epoch 149/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.0763 - accuracy: 0.6571 - val_loss: 1.1661 - val_accuracy: 0.7181\n",
            "Epoch 150/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.3621 - accuracy: 0.6368 - val_loss: 1.3931 - val_accuracy: 0.6064\n",
            "Epoch 151/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.9365 - accuracy: 0.6742 - val_loss: 0.6762 - val_accuracy: 0.7340\n",
            "Epoch 152/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.8248 - accuracy: 0.6810 - val_loss: 1.2448 - val_accuracy: 0.6649\n",
            "Epoch 153/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8178 - accuracy: 0.6966 - val_loss: 0.6870 - val_accuracy: 0.7766\n",
            "Epoch 154/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 1.0879 - accuracy: 0.6694 - val_loss: 0.6755 - val_accuracy: 0.7181\n",
            "Epoch 155/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.0979 - accuracy: 0.6803 - val_loss: 0.7866 - val_accuracy: 0.7553\n",
            "Epoch 156/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.0614 - accuracy: 0.6502 - val_loss: 1.0763 - val_accuracy: 0.6702\n",
            "Epoch 157/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.1426 - accuracy: 0.6535 - val_loss: 0.9139 - val_accuracy: 0.7181\n",
            "Epoch 158/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.1145 - accuracy: 0.6426 - val_loss: 1.3662 - val_accuracy: 0.6011\n",
            "Epoch 159/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.2720 - accuracy: 0.6165 - val_loss: 0.7588 - val_accuracy: 0.7660\n",
            "Epoch 160/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.0801 - accuracy: 0.6495 - val_loss: 0.7961 - val_accuracy: 0.7447\n",
            "Epoch 161/8000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.8122 - accuracy: 0.6908 - val_loss: 0.8519 - val_accuracy: 0.7128\n",
            "Epoch 162/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8763 - accuracy: 0.6850 - val_loss: 1.1774 - val_accuracy: 0.6649\n",
            "Epoch 163/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.9082 - accuracy: 0.6948 - val_loss: 2.9469 - val_accuracy: 0.6064\n",
            "Epoch 164/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.9805 - accuracy: 0.6216 - val_loss: 0.8360 - val_accuracy: 0.7447\n",
            "Epoch 165/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.9112 - accuracy: 0.6694 - val_loss: 1.5357 - val_accuracy: 0.7074\n",
            "Epoch 166/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.9574 - accuracy: 0.6694 - val_loss: 0.9977 - val_accuracy: 0.6862\n",
            "Epoch 167/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7236 - accuracy: 0.7061 - val_loss: 0.6252 - val_accuracy: 0.7500\n",
            "Epoch 168/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7542 - accuracy: 0.6923 - val_loss: 0.5876 - val_accuracy: 0.7553\n",
            "Epoch 169/8000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.8713 - accuracy: 0.6999 - val_loss: 0.8372 - val_accuracy: 0.7287\n",
            "Epoch 170/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.1823 - accuracy: 0.6397 - val_loss: 0.9580 - val_accuracy: 0.7021\n",
            "Epoch 171/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7824 - accuracy: 0.7039 - val_loss: 0.8262 - val_accuracy: 0.7394\n",
            "Epoch 172/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.0686 - accuracy: 0.6586 - val_loss: 0.8702 - val_accuracy: 0.7181\n",
            "Epoch 173/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.9293 - accuracy: 0.6843 - val_loss: 1.3221 - val_accuracy: 0.6968\n",
            "Epoch 174/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.2999 - accuracy: 0.6430 - val_loss: 1.3575 - val_accuracy: 0.6330\n",
            "Epoch 175/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.2377 - accuracy: 0.6379 - val_loss: 1.3961 - val_accuracy: 0.6330\n",
            "Epoch 176/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8801 - accuracy: 0.6713 - val_loss: 0.8889 - val_accuracy: 0.7394\n",
            "Epoch 177/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7839 - accuracy: 0.6908 - val_loss: 0.6805 - val_accuracy: 0.7500\n",
            "Epoch 178/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8494 - accuracy: 0.6673 - val_loss: 0.7826 - val_accuracy: 0.7074\n",
            "Epoch 179/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.8354 - accuracy: 0.6774 - val_loss: 0.6048 - val_accuracy: 0.7553\n",
            "Epoch 180/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7146 - accuracy: 0.7079 - val_loss: 0.5682 - val_accuracy: 0.7766\n",
            "Epoch 181/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8801 - accuracy: 0.6756 - val_loss: 1.5433 - val_accuracy: 0.7128\n",
            "Epoch 182/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.7295 - accuracy: 0.5966 - val_loss: 1.9383 - val_accuracy: 0.5798\n",
            "Epoch 183/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.4466 - accuracy: 0.6408 - val_loss: 0.6649 - val_accuracy: 0.7340\n",
            "Epoch 184/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.5169 - accuracy: 0.5883 - val_loss: 1.4654 - val_accuracy: 0.6755\n",
            "Epoch 185/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.1426 - accuracy: 0.6589 - val_loss: 1.1116 - val_accuracy: 0.7234\n",
            "Epoch 186/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.2150 - accuracy: 0.6491 - val_loss: 0.8839 - val_accuracy: 0.7447\n",
            "Epoch 187/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.3547 - accuracy: 0.6158 - val_loss: 1.5011 - val_accuracy: 0.6649\n",
            "Epoch 188/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8861 - accuracy: 0.6941 - val_loss: 1.0584 - val_accuracy: 0.7021\n",
            "Epoch 189/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.9506 - accuracy: 0.6622 - val_loss: 0.8346 - val_accuracy: 0.7713\n",
            "Epoch 190/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7877 - accuracy: 0.6908 - val_loss: 1.5152 - val_accuracy: 0.7234\n",
            "Epoch 191/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.9182 - accuracy: 0.6934 - val_loss: 0.9180 - val_accuracy: 0.6915\n",
            "Epoch 192/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.4062 - accuracy: 0.6104 - val_loss: 2.0103 - val_accuracy: 0.6702\n",
            "Epoch 193/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 2.1430 - accuracy: 0.6060 - val_loss: 0.8951 - val_accuracy: 0.7606\n",
            "Epoch 194/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.0876 - accuracy: 0.6647 - val_loss: 1.4199 - val_accuracy: 0.6436\n",
            "Epoch 195/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.9057 - accuracy: 0.6611 - val_loss: 0.8628 - val_accuracy: 0.6862\n",
            "Epoch 196/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8428 - accuracy: 0.6749 - val_loss: 0.6808 - val_accuracy: 0.7181\n",
            "Epoch 197/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7352 - accuracy: 0.6916 - val_loss: 0.8630 - val_accuracy: 0.6968\n",
            "Epoch 198/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8392 - accuracy: 0.6684 - val_loss: 0.9394 - val_accuracy: 0.7394\n",
            "Epoch 199/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.2127 - accuracy: 0.6350 - val_loss: 0.9253 - val_accuracy: 0.7394\n",
            "Epoch 200/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8610 - accuracy: 0.6876 - val_loss: 0.6551 - val_accuracy: 0.7553\n",
            "Epoch 201/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7080 - accuracy: 0.7039 - val_loss: 1.4075 - val_accuracy: 0.7128\n",
            "Epoch 202/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8760 - accuracy: 0.6694 - val_loss: 0.9986 - val_accuracy: 0.7234\n",
            "Epoch 203/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.1810 - accuracy: 0.6499 - val_loss: 1.6652 - val_accuracy: 0.6862\n",
            "Epoch 204/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.0997 - accuracy: 0.6270 - val_loss: 1.1652 - val_accuracy: 0.7181\n",
            "Epoch 205/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.8038 - accuracy: 0.6963 - val_loss: 0.5656 - val_accuracy: 0.7713\n",
            "Epoch 206/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6008 - accuracy: 0.7274 - val_loss: 0.5767 - val_accuracy: 0.7500\n",
            "Epoch 207/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7129 - accuracy: 0.6887 - val_loss: 0.5900 - val_accuracy: 0.7340\n",
            "Epoch 208/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.6988 - accuracy: 0.6263 - val_loss: 1.9103 - val_accuracy: 0.5638\n",
            "Epoch 209/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.1365 - accuracy: 0.6586 - val_loss: 1.0519 - val_accuracy: 0.7021\n",
            "Epoch 210/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7287 - accuracy: 0.6948 - val_loss: 0.5726 - val_accuracy: 0.7447\n",
            "Epoch 211/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7272 - accuracy: 0.6916 - val_loss: 0.6701 - val_accuracy: 0.7500\n",
            "Epoch 212/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6341 - accuracy: 0.7231 - val_loss: 0.9035 - val_accuracy: 0.7287\n",
            "Epoch 213/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8030 - accuracy: 0.6781 - val_loss: 2.1032 - val_accuracy: 0.6702\n",
            "Epoch 214/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7987 - accuracy: 0.6897 - val_loss: 0.7837 - val_accuracy: 0.7181\n",
            "Epoch 215/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6461 - accuracy: 0.7162 - val_loss: 0.7658 - val_accuracy: 0.7500\n",
            "Epoch 216/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.9812 - accuracy: 0.6433 - val_loss: 0.9847 - val_accuracy: 0.6596\n",
            "Epoch 217/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.9003 - accuracy: 0.6832 - val_loss: 0.5839 - val_accuracy: 0.7606\n",
            "Epoch 218/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8475 - accuracy: 0.6829 - val_loss: 0.8334 - val_accuracy: 0.7234\n",
            "Epoch 219/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7488 - accuracy: 0.6919 - val_loss: 0.6299 - val_accuracy: 0.7500\n",
            "Epoch 220/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.8386 - accuracy: 0.6839 - val_loss: 1.1498 - val_accuracy: 0.6915\n",
            "Epoch 221/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.9560 - accuracy: 0.6658 - val_loss: 0.7006 - val_accuracy: 0.7340\n",
            "Epoch 222/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7537 - accuracy: 0.6720 - val_loss: 0.6615 - val_accuracy: 0.7234\n",
            "Epoch 223/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6762 - accuracy: 0.6963 - val_loss: 0.5321 - val_accuracy: 0.7553\n",
            "Epoch 224/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7134 - accuracy: 0.7133 - val_loss: 0.6770 - val_accuracy: 0.7340\n",
            "Epoch 225/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.3794 - accuracy: 0.6028 - val_loss: 0.6080 - val_accuracy: 0.7660\n",
            "Epoch 226/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8459 - accuracy: 0.6836 - val_loss: 0.7272 - val_accuracy: 0.7394\n",
            "Epoch 227/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8125 - accuracy: 0.6723 - val_loss: 1.1293 - val_accuracy: 0.6755\n",
            "Epoch 228/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7628 - accuracy: 0.6887 - val_loss: 0.6521 - val_accuracy: 0.7394\n",
            "Epoch 229/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6741 - accuracy: 0.6970 - val_loss: 0.6828 - val_accuracy: 0.7181\n",
            "Epoch 230/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6773 - accuracy: 0.6977 - val_loss: 0.7409 - val_accuracy: 0.7553\n",
            "Epoch 231/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.8614 - accuracy: 0.6669 - val_loss: 0.6199 - val_accuracy: 0.7606\n",
            "Epoch 232/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7254 - accuracy: 0.6948 - val_loss: 0.5403 - val_accuracy: 0.7713\n",
            "Epoch 233/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.9260 - accuracy: 0.6575 - val_loss: 1.1299 - val_accuracy: 0.6809\n",
            "Epoch 234/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6876 - accuracy: 0.6988 - val_loss: 0.6837 - val_accuracy: 0.7606\n",
            "Epoch 235/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7648 - accuracy: 0.6662 - val_loss: 0.6366 - val_accuracy: 0.7340\n",
            "Epoch 236/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6772 - accuracy: 0.7032 - val_loss: 0.6733 - val_accuracy: 0.7606\n",
            "Epoch 237/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7796 - accuracy: 0.6738 - val_loss: 0.7749 - val_accuracy: 0.7181\n",
            "Epoch 238/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6825 - accuracy: 0.7151 - val_loss: 0.5627 - val_accuracy: 0.7713\n",
            "Epoch 239/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6255 - accuracy: 0.7075 - val_loss: 0.5549 - val_accuracy: 0.7660\n",
            "Epoch 240/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6071 - accuracy: 0.7249 - val_loss: 0.9015 - val_accuracy: 0.6809\n",
            "Epoch 241/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.8746 - accuracy: 0.6705 - val_loss: 0.7642 - val_accuracy: 0.7606\n",
            "Epoch 242/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7846 - accuracy: 0.6901 - val_loss: 1.0119 - val_accuracy: 0.7394\n",
            "Epoch 243/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.9194 - accuracy: 0.6506 - val_loss: 0.5399 - val_accuracy: 0.7660\n",
            "Epoch 244/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8235 - accuracy: 0.6669 - val_loss: 0.7742 - val_accuracy: 0.7181\n",
            "Epoch 245/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7064 - accuracy: 0.6876 - val_loss: 1.0879 - val_accuracy: 0.6702\n",
            "Epoch 246/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7488 - accuracy: 0.6774 - val_loss: 0.5960 - val_accuracy: 0.7074\n",
            "Epoch 247/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.9406 - accuracy: 0.6455 - val_loss: 1.6823 - val_accuracy: 0.5319\n",
            "Epoch 248/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.9800 - accuracy: 0.6742 - val_loss: 0.8165 - val_accuracy: 0.7287\n",
            "Epoch 249/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7880 - accuracy: 0.6774 - val_loss: 0.9072 - val_accuracy: 0.7394\n",
            "Epoch 250/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.9080 - accuracy: 0.6604 - val_loss: 1.0174 - val_accuracy: 0.6809\n",
            "Epoch 251/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7823 - accuracy: 0.6926 - val_loss: 0.7147 - val_accuracy: 0.7234\n",
            "Epoch 252/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6334 - accuracy: 0.7129 - val_loss: 0.8066 - val_accuracy: 0.7234\n",
            "Epoch 253/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8155 - accuracy: 0.6865 - val_loss: 0.6762 - val_accuracy: 0.7394\n",
            "Epoch 254/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6790 - accuracy: 0.6999 - val_loss: 0.5985 - val_accuracy: 0.7606\n",
            "Epoch 255/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7145 - accuracy: 0.6810 - val_loss: 0.6366 - val_accuracy: 0.7447\n",
            "Epoch 256/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6261 - accuracy: 0.7187 - val_loss: 0.6830 - val_accuracy: 0.7181\n",
            "Epoch 257/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8427 - accuracy: 0.6858 - val_loss: 0.6327 - val_accuracy: 0.7287\n",
            "Epoch 258/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6833 - accuracy: 0.7108 - val_loss: 1.0034 - val_accuracy: 0.6702\n",
            "Epoch 259/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8096 - accuracy: 0.6981 - val_loss: 0.7578 - val_accuracy: 0.7606\n",
            "Epoch 260/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6263 - accuracy: 0.7289 - val_loss: 0.5795 - val_accuracy: 0.7660\n",
            "Epoch 261/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6539 - accuracy: 0.7090 - val_loss: 0.7857 - val_accuracy: 0.7128\n",
            "Epoch 262/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6230 - accuracy: 0.7126 - val_loss: 0.5916 - val_accuracy: 0.7606\n",
            "Epoch 263/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.9798 - accuracy: 0.6455 - val_loss: 1.7137 - val_accuracy: 0.6277\n",
            "Epoch 264/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8752 - accuracy: 0.6611 - val_loss: 1.0284 - val_accuracy: 0.6543\n",
            "Epoch 265/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7388 - accuracy: 0.6930 - val_loss: 0.7741 - val_accuracy: 0.6915\n",
            "Epoch 266/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6605 - accuracy: 0.6908 - val_loss: 0.5917 - val_accuracy: 0.7606\n",
            "Epoch 267/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6523 - accuracy: 0.6952 - val_loss: 0.5210 - val_accuracy: 0.7660\n",
            "Epoch 268/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 1.0011 - accuracy: 0.6568 - val_loss: 1.2211 - val_accuracy: 0.7128\n",
            "Epoch 269/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7756 - accuracy: 0.6883 - val_loss: 0.5781 - val_accuracy: 0.7553\n",
            "Epoch 270/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7100 - accuracy: 0.6984 - val_loss: 0.8749 - val_accuracy: 0.7340\n",
            "Epoch 271/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7322 - accuracy: 0.6858 - val_loss: 0.7942 - val_accuracy: 0.6755\n",
            "Epoch 272/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.8340 - accuracy: 0.6713 - val_loss: 0.9750 - val_accuracy: 0.6702\n",
            "Epoch 273/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.0871 - accuracy: 0.6296 - val_loss: 0.7979 - val_accuracy: 0.7074\n",
            "Epoch 274/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6580 - accuracy: 0.7061 - val_loss: 0.7103 - val_accuracy: 0.7394\n",
            "Epoch 275/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.6309 - accuracy: 0.7144 - val_loss: 0.5982 - val_accuracy: 0.7660\n",
            "Epoch 276/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6147 - accuracy: 0.7180 - val_loss: 0.5319 - val_accuracy: 0.7606\n",
            "Epoch 277/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7224 - accuracy: 0.6767 - val_loss: 1.3036 - val_accuracy: 0.6862\n",
            "Epoch 278/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7609 - accuracy: 0.6912 - val_loss: 0.5439 - val_accuracy: 0.7713\n",
            "Epoch 279/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5866 - accuracy: 0.7144 - val_loss: 0.9231 - val_accuracy: 0.6915\n",
            "Epoch 280/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7518 - accuracy: 0.6887 - val_loss: 0.7414 - val_accuracy: 0.7287\n",
            "Epoch 281/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5980 - accuracy: 0.7282 - val_loss: 0.7087 - val_accuracy: 0.7021\n",
            "Epoch 282/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5925 - accuracy: 0.7177 - val_loss: 0.6232 - val_accuracy: 0.7553\n",
            "Epoch 283/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7109 - accuracy: 0.6861 - val_loss: 0.5470 - val_accuracy: 0.7713\n",
            "Epoch 284/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5620 - accuracy: 0.7376 - val_loss: 0.5468 - val_accuracy: 0.7500\n",
            "Epoch 285/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.5666 - accuracy: 0.7412 - val_loss: 0.5062 - val_accuracy: 0.7872\n",
            "Epoch 286/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5986 - accuracy: 0.7387 - val_loss: 0.7279 - val_accuracy: 0.7340\n",
            "Epoch 287/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.8287 - accuracy: 0.6832 - val_loss: 0.5863 - val_accuracy: 0.7447\n",
            "Epoch 288/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7562 - accuracy: 0.6821 - val_loss: 0.9393 - val_accuracy: 0.6702\n",
            "Epoch 289/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6800 - accuracy: 0.7032 - val_loss: 0.7006 - val_accuracy: 0.7287\n",
            "Epoch 290/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6651 - accuracy: 0.7075 - val_loss: 0.6528 - val_accuracy: 0.7553\n",
            "Epoch 291/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5997 - accuracy: 0.7137 - val_loss: 0.6118 - val_accuracy: 0.7234\n",
            "Epoch 292/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6330 - accuracy: 0.7003 - val_loss: 0.5981 - val_accuracy: 0.7500\n",
            "Epoch 293/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8632 - accuracy: 0.6662 - val_loss: 0.7610 - val_accuracy: 0.7287\n",
            "Epoch 294/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6545 - accuracy: 0.7021 - val_loss: 0.5617 - val_accuracy: 0.7340\n",
            "Epoch 295/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.9166 - accuracy: 0.6466 - val_loss: 0.8327 - val_accuracy: 0.7128\n",
            "Epoch 296/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.8628 - accuracy: 0.6713 - val_loss: 0.7084 - val_accuracy: 0.7500\n",
            "Epoch 297/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7578 - accuracy: 0.6858 - val_loss: 1.0319 - val_accuracy: 0.6277\n",
            "Epoch 298/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7967 - accuracy: 0.6767 - val_loss: 0.5765 - val_accuracy: 0.7500\n",
            "Epoch 299/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6163 - accuracy: 0.7187 - val_loss: 0.5147 - val_accuracy: 0.7660\n",
            "Epoch 300/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6199 - accuracy: 0.7035 - val_loss: 1.0160 - val_accuracy: 0.6809\n",
            "Epoch 301/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7548 - accuracy: 0.6763 - val_loss: 0.7248 - val_accuracy: 0.7394\n",
            "Epoch 302/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6056 - accuracy: 0.7242 - val_loss: 0.5769 - val_accuracy: 0.7447\n",
            "Epoch 303/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6425 - accuracy: 0.7042 - val_loss: 0.6788 - val_accuracy: 0.7287\n",
            "Epoch 304/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5885 - accuracy: 0.7177 - val_loss: 0.6042 - val_accuracy: 0.7340\n",
            "Epoch 305/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5845 - accuracy: 0.7292 - val_loss: 0.5282 - val_accuracy: 0.7606\n",
            "Epoch 306/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5894 - accuracy: 0.7249 - val_loss: 0.6394 - val_accuracy: 0.7234\n",
            "Epoch 307/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6184 - accuracy: 0.7151 - val_loss: 0.6003 - val_accuracy: 0.7128\n",
            "Epoch 308/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6006 - accuracy: 0.7166 - val_loss: 0.6294 - val_accuracy: 0.7394\n",
            "Epoch 309/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6180 - accuracy: 0.7079 - val_loss: 0.7339 - val_accuracy: 0.6968\n",
            "Epoch 310/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7898 - accuracy: 0.6734 - val_loss: 1.0184 - val_accuracy: 0.6649\n",
            "Epoch 311/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6604 - accuracy: 0.6916 - val_loss: 0.5412 - val_accuracy: 0.7606\n",
            "Epoch 312/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5812 - accuracy: 0.7166 - val_loss: 0.5284 - val_accuracy: 0.7606\n",
            "Epoch 313/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6497 - accuracy: 0.7046 - val_loss: 0.8985 - val_accuracy: 0.6755\n",
            "Epoch 314/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7271 - accuracy: 0.6872 - val_loss: 0.5539 - val_accuracy: 0.7500\n",
            "Epoch 315/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6336 - accuracy: 0.7169 - val_loss: 0.5703 - val_accuracy: 0.7394\n",
            "Epoch 316/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5673 - accuracy: 0.7340 - val_loss: 0.5487 - val_accuracy: 0.7500\n",
            "Epoch 317/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6059 - accuracy: 0.7162 - val_loss: 0.5590 - val_accuracy: 0.7500\n",
            "Epoch 318/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5493 - accuracy: 0.7358 - val_loss: 0.5062 - val_accuracy: 0.7340\n",
            "Epoch 319/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6188 - accuracy: 0.7090 - val_loss: 0.6320 - val_accuracy: 0.7713\n",
            "Epoch 320/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5771 - accuracy: 0.7216 - val_loss: 0.7309 - val_accuracy: 0.6968\n",
            "Epoch 321/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6355 - accuracy: 0.7119 - val_loss: 0.5710 - val_accuracy: 0.7713\n",
            "Epoch 322/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.8906 - accuracy: 0.6578 - val_loss: 0.5683 - val_accuracy: 0.7394\n",
            "Epoch 323/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7239 - accuracy: 0.6912 - val_loss: 0.5361 - val_accuracy: 0.7340\n",
            "Epoch 324/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6451 - accuracy: 0.7108 - val_loss: 0.5280 - val_accuracy: 0.7713\n",
            "Epoch 325/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5697 - accuracy: 0.7329 - val_loss: 0.5083 - val_accuracy: 0.7606\n",
            "Epoch 326/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5488 - accuracy: 0.7369 - val_loss: 0.5026 - val_accuracy: 0.7606\n",
            "Epoch 327/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.6363 - accuracy: 0.7039 - val_loss: 0.4902 - val_accuracy: 0.7713\n",
            "Epoch 328/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6226 - accuracy: 0.7140 - val_loss: 0.5617 - val_accuracy: 0.7340\n",
            "Epoch 329/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5615 - accuracy: 0.7376 - val_loss: 0.8298 - val_accuracy: 0.6968\n",
            "Epoch 330/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6121 - accuracy: 0.7111 - val_loss: 0.5031 - val_accuracy: 0.7872\n",
            "Epoch 331/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5910 - accuracy: 0.7126 - val_loss: 0.5209 - val_accuracy: 0.7819\n",
            "Epoch 332/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5791 - accuracy: 0.7173 - val_loss: 0.5263 - val_accuracy: 0.7660\n",
            "Epoch 333/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5458 - accuracy: 0.7387 - val_loss: 0.5676 - val_accuracy: 0.7394\n",
            "Epoch 334/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6205 - accuracy: 0.7017 - val_loss: 0.8065 - val_accuracy: 0.6649\n",
            "Epoch 335/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6355 - accuracy: 0.7173 - val_loss: 0.5510 - val_accuracy: 0.7340\n",
            "Epoch 336/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6096 - accuracy: 0.7162 - val_loss: 0.5683 - val_accuracy: 0.7287\n",
            "Epoch 337/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6236 - accuracy: 0.7090 - val_loss: 0.7135 - val_accuracy: 0.7021\n",
            "Epoch 338/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.8332 - accuracy: 0.6669 - val_loss: 0.5663 - val_accuracy: 0.7713\n",
            "Epoch 339/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7136 - accuracy: 0.6887 - val_loss: 0.5483 - val_accuracy: 0.7713\n",
            "Epoch 340/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6336 - accuracy: 0.6974 - val_loss: 0.5180 - val_accuracy: 0.7660\n",
            "Epoch 341/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5615 - accuracy: 0.7191 - val_loss: 0.6678 - val_accuracy: 0.6862\n",
            "Epoch 342/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6240 - accuracy: 0.7158 - val_loss: 0.4985 - val_accuracy: 0.7926\n",
            "Epoch 343/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5626 - accuracy: 0.7292 - val_loss: 0.6729 - val_accuracy: 0.7234\n",
            "Epoch 344/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5552 - accuracy: 0.7340 - val_loss: 0.5081 - val_accuracy: 0.7394\n",
            "Epoch 345/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5642 - accuracy: 0.7303 - val_loss: 0.5031 - val_accuracy: 0.7766\n",
            "Epoch 346/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5913 - accuracy: 0.7155 - val_loss: 0.5257 - val_accuracy: 0.7447\n",
            "Epoch 347/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5685 - accuracy: 0.7253 - val_loss: 0.5886 - val_accuracy: 0.7553\n",
            "Epoch 348/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6319 - accuracy: 0.7097 - val_loss: 0.5946 - val_accuracy: 0.7447\n",
            "Epoch 349/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5388 - accuracy: 0.7434 - val_loss: 0.5361 - val_accuracy: 0.7553\n",
            "Epoch 350/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5690 - accuracy: 0.7300 - val_loss: 0.5035 - val_accuracy: 0.7447\n",
            "Epoch 351/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5513 - accuracy: 0.7434 - val_loss: 0.6334 - val_accuracy: 0.7553\n",
            "Epoch 352/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7324 - accuracy: 0.6832 - val_loss: 0.9006 - val_accuracy: 0.7128\n",
            "Epoch 353/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.7032 - accuracy: 0.6934 - val_loss: 0.6529 - val_accuracy: 0.7447\n",
            "Epoch 354/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5782 - accuracy: 0.7126 - val_loss: 0.7446 - val_accuracy: 0.6968\n",
            "Epoch 355/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6028 - accuracy: 0.7209 - val_loss: 0.6199 - val_accuracy: 0.7287\n",
            "Epoch 356/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6243 - accuracy: 0.7151 - val_loss: 0.5594 - val_accuracy: 0.7713\n",
            "Epoch 357/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5721 - accuracy: 0.7264 - val_loss: 0.5494 - val_accuracy: 0.7553\n",
            "Epoch 358/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6111 - accuracy: 0.7126 - val_loss: 0.6009 - val_accuracy: 0.7021\n",
            "Epoch 359/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5597 - accuracy: 0.7376 - val_loss: 0.5125 - val_accuracy: 0.7606\n",
            "Epoch 360/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5434 - accuracy: 0.7365 - val_loss: 0.4961 - val_accuracy: 0.7606\n",
            "Epoch 361/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5574 - accuracy: 0.7289 - val_loss: 0.4940 - val_accuracy: 0.7979\n",
            "Epoch 362/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5920 - accuracy: 0.7111 - val_loss: 0.6125 - val_accuracy: 0.7447\n",
            "Epoch 363/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5815 - accuracy: 0.7209 - val_loss: 0.6506 - val_accuracy: 0.7500\n",
            "Epoch 364/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5820 - accuracy: 0.7071 - val_loss: 0.5534 - val_accuracy: 0.7394\n",
            "Epoch 365/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6067 - accuracy: 0.7075 - val_loss: 0.6445 - val_accuracy: 0.7340\n",
            "Epoch 366/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6639 - accuracy: 0.6974 - val_loss: 0.5620 - val_accuracy: 0.7394\n",
            "Epoch 367/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5984 - accuracy: 0.7227 - val_loss: 0.4928 - val_accuracy: 0.7766\n",
            "Epoch 368/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5600 - accuracy: 0.7292 - val_loss: 0.5436 - val_accuracy: 0.7500\n",
            "Epoch 369/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5648 - accuracy: 0.7224 - val_loss: 0.6167 - val_accuracy: 0.7340\n",
            "Epoch 370/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6860 - accuracy: 0.7053 - val_loss: 0.6612 - val_accuracy: 0.7447\n",
            "Epoch 371/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5700 - accuracy: 0.7180 - val_loss: 0.5313 - val_accuracy: 0.7394\n",
            "Epoch 372/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5801 - accuracy: 0.7191 - val_loss: 0.6914 - val_accuracy: 0.6968\n",
            "Epoch 373/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6311 - accuracy: 0.7032 - val_loss: 0.8916 - val_accuracy: 0.7128\n",
            "Epoch 374/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6176 - accuracy: 0.7071 - val_loss: 0.5037 - val_accuracy: 0.7606\n",
            "Epoch 375/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5412 - accuracy: 0.7361 - val_loss: 0.6046 - val_accuracy: 0.7606\n",
            "Epoch 376/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5844 - accuracy: 0.7119 - val_loss: 0.5097 - val_accuracy: 0.7500\n",
            "Epoch 377/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5858 - accuracy: 0.7267 - val_loss: 0.5066 - val_accuracy: 0.7606\n",
            "Epoch 378/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5711 - accuracy: 0.7343 - val_loss: 0.4926 - val_accuracy: 0.7606\n",
            "Epoch 379/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5737 - accuracy: 0.7253 - val_loss: 0.5656 - val_accuracy: 0.7500\n",
            "Epoch 380/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5499 - accuracy: 0.7325 - val_loss: 0.5074 - val_accuracy: 0.7660\n",
            "Epoch 381/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5850 - accuracy: 0.7166 - val_loss: 0.5140 - val_accuracy: 0.7660\n",
            "Epoch 382/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5451 - accuracy: 0.7419 - val_loss: 0.6336 - val_accuracy: 0.7394\n",
            "Epoch 383/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5981 - accuracy: 0.7111 - val_loss: 0.6612 - val_accuracy: 0.7553\n",
            "Epoch 384/8000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.5601 - accuracy: 0.7267 - val_loss: 0.5160 - val_accuracy: 0.7872\n",
            "Epoch 385/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5364 - accuracy: 0.7427 - val_loss: 0.5226 - val_accuracy: 0.7500\n",
            "Epoch 386/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5490 - accuracy: 0.7311 - val_loss: 0.5239 - val_accuracy: 0.7606\n",
            "Epoch 387/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6425 - accuracy: 0.7148 - val_loss: 0.5288 - val_accuracy: 0.7447\n",
            "Epoch 388/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6714 - accuracy: 0.6981 - val_loss: 0.6051 - val_accuracy: 0.7074\n",
            "Epoch 389/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5950 - accuracy: 0.7209 - val_loss: 0.5245 - val_accuracy: 0.7128\n",
            "Epoch 390/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5439 - accuracy: 0.7361 - val_loss: 0.5656 - val_accuracy: 0.7447\n",
            "Epoch 391/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6242 - accuracy: 0.7173 - val_loss: 0.5199 - val_accuracy: 0.7394\n",
            "Epoch 392/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5240 - accuracy: 0.7437 - val_loss: 0.5228 - val_accuracy: 0.7713\n",
            "Epoch 393/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5620 - accuracy: 0.7220 - val_loss: 0.8157 - val_accuracy: 0.6968\n",
            "Epoch 394/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5629 - accuracy: 0.7336 - val_loss: 0.5136 - val_accuracy: 0.7287\n",
            "Epoch 395/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5462 - accuracy: 0.7336 - val_loss: 0.5073 - val_accuracy: 0.7500\n",
            "Epoch 396/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5569 - accuracy: 0.7398 - val_loss: 0.5894 - val_accuracy: 0.7500\n",
            "Epoch 397/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5942 - accuracy: 0.7133 - val_loss: 0.4931 - val_accuracy: 0.7872\n",
            "Epoch 398/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5213 - accuracy: 0.7437 - val_loss: 0.5467 - val_accuracy: 0.7553\n",
            "Epoch 399/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5535 - accuracy: 0.7216 - val_loss: 0.7835 - val_accuracy: 0.6862\n",
            "Epoch 400/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5907 - accuracy: 0.7097 - val_loss: 0.4970 - val_accuracy: 0.7394\n",
            "Epoch 401/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5636 - accuracy: 0.7198 - val_loss: 0.5195 - val_accuracy: 0.7394\n",
            "Epoch 402/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5440 - accuracy: 0.7358 - val_loss: 0.5153 - val_accuracy: 0.7500\n",
            "Epoch 403/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5443 - accuracy: 0.7314 - val_loss: 0.5796 - val_accuracy: 0.7606\n",
            "Epoch 404/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5359 - accuracy: 0.7416 - val_loss: 0.5309 - val_accuracy: 0.7447\n",
            "Epoch 405/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.5565 - accuracy: 0.7202 - val_loss: 0.5331 - val_accuracy: 0.7394\n",
            "Epoch 406/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5299 - accuracy: 0.7430 - val_loss: 0.5317 - val_accuracy: 0.7606\n",
            "Epoch 407/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5473 - accuracy: 0.7321 - val_loss: 0.5078 - val_accuracy: 0.7660\n",
            "Epoch 408/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5458 - accuracy: 0.7350 - val_loss: 0.5112 - val_accuracy: 0.7500\n",
            "Epoch 409/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5643 - accuracy: 0.7311 - val_loss: 0.5080 - val_accuracy: 0.7553\n",
            "Epoch 410/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5659 - accuracy: 0.7187 - val_loss: 0.5460 - val_accuracy: 0.7394\n",
            "Epoch 411/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5339 - accuracy: 0.7419 - val_loss: 0.5427 - val_accuracy: 0.7606\n",
            "Epoch 412/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5579 - accuracy: 0.7249 - val_loss: 0.4895 - val_accuracy: 0.7713\n",
            "Epoch 413/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5596 - accuracy: 0.7278 - val_loss: 0.5517 - val_accuracy: 0.7340\n",
            "Epoch 414/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5444 - accuracy: 0.7282 - val_loss: 0.5392 - val_accuracy: 0.7447\n",
            "Epoch 415/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5498 - accuracy: 0.7369 - val_loss: 0.6856 - val_accuracy: 0.7287\n",
            "Epoch 416/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7074 - accuracy: 0.6684 - val_loss: 0.5318 - val_accuracy: 0.7447\n",
            "Epoch 417/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5899 - accuracy: 0.7238 - val_loss: 0.6816 - val_accuracy: 0.7128\n",
            "Epoch 418/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5559 - accuracy: 0.7387 - val_loss: 0.6533 - val_accuracy: 0.6755\n",
            "Epoch 419/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5779 - accuracy: 0.7282 - val_loss: 0.5260 - val_accuracy: 0.7394\n",
            "Epoch 420/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5471 - accuracy: 0.7307 - val_loss: 0.5328 - val_accuracy: 0.7713\n",
            "Epoch 421/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5833 - accuracy: 0.7169 - val_loss: 0.5073 - val_accuracy: 0.7766\n",
            "Epoch 422/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5865 - accuracy: 0.7100 - val_loss: 0.6204 - val_accuracy: 0.7340\n",
            "Epoch 423/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5776 - accuracy: 0.7260 - val_loss: 0.5471 - val_accuracy: 0.7447\n",
            "Epoch 424/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5362 - accuracy: 0.7387 - val_loss: 0.5328 - val_accuracy: 0.7447\n",
            "Epoch 425/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5313 - accuracy: 0.7383 - val_loss: 0.5269 - val_accuracy: 0.7447\n",
            "Epoch 426/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5296 - accuracy: 0.7408 - val_loss: 0.5209 - val_accuracy: 0.7340\n",
            "Epoch 427/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5358 - accuracy: 0.7401 - val_loss: 0.5091 - val_accuracy: 0.7606\n",
            "Epoch 428/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5423 - accuracy: 0.7354 - val_loss: 0.5200 - val_accuracy: 0.7181\n",
            "Epoch 429/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5753 - accuracy: 0.7227 - val_loss: 0.5853 - val_accuracy: 0.7553\n",
            "Epoch 430/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.6551 - accuracy: 0.6981 - val_loss: 0.6393 - val_accuracy: 0.6862\n",
            "Epoch 431/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5870 - accuracy: 0.7173 - val_loss: 0.5789 - val_accuracy: 0.7074\n",
            "Epoch 432/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5368 - accuracy: 0.7401 - val_loss: 0.5065 - val_accuracy: 0.7447\n",
            "Epoch 433/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5506 - accuracy: 0.7343 - val_loss: 0.5057 - val_accuracy: 0.7660\n",
            "Epoch 434/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5381 - accuracy: 0.7372 - val_loss: 0.5210 - val_accuracy: 0.7287\n",
            "Epoch 435/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5304 - accuracy: 0.7347 - val_loss: 0.5585 - val_accuracy: 0.7500\n",
            "Epoch 436/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5237 - accuracy: 0.7492 - val_loss: 0.5218 - val_accuracy: 0.7340\n",
            "Epoch 437/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5171 - accuracy: 0.7477 - val_loss: 0.5256 - val_accuracy: 0.7500\n",
            "Epoch 438/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5118 - accuracy: 0.7521 - val_loss: 0.5591 - val_accuracy: 0.7553\n",
            "Epoch 439/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5648 - accuracy: 0.7249 - val_loss: 0.5765 - val_accuracy: 0.7394\n",
            "Epoch 440/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6217 - accuracy: 0.7035 - val_loss: 0.7162 - val_accuracy: 0.6489\n",
            "Epoch 441/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5947 - accuracy: 0.7090 - val_loss: 0.5176 - val_accuracy: 0.7606\n",
            "Epoch 442/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5357 - accuracy: 0.7441 - val_loss: 0.4984 - val_accuracy: 0.7766\n",
            "Epoch 443/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5320 - accuracy: 0.7423 - val_loss: 0.5635 - val_accuracy: 0.7340\n",
            "Epoch 444/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5395 - accuracy: 0.7347 - val_loss: 0.5379 - val_accuracy: 0.7447\n",
            "Epoch 445/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5637 - accuracy: 0.7191 - val_loss: 0.5900 - val_accuracy: 0.7394\n",
            "Epoch 446/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5443 - accuracy: 0.7264 - val_loss: 0.5288 - val_accuracy: 0.7660\n",
            "Epoch 447/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5313 - accuracy: 0.7528 - val_loss: 0.5384 - val_accuracy: 0.7553\n",
            "Epoch 448/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5464 - accuracy: 0.7206 - val_loss: 0.5380 - val_accuracy: 0.7394\n",
            "Epoch 449/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5776 - accuracy: 0.7231 - val_loss: 0.5112 - val_accuracy: 0.7394\n",
            "Epoch 450/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5180 - accuracy: 0.7405 - val_loss: 0.5322 - val_accuracy: 0.7500\n",
            "Epoch 451/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5133 - accuracy: 0.7437 - val_loss: 0.5473 - val_accuracy: 0.7713\n",
            "Epoch 452/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5485 - accuracy: 0.7329 - val_loss: 0.5124 - val_accuracy: 0.7606\n",
            "Epoch 453/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5198 - accuracy: 0.7437 - val_loss: 0.5278 - val_accuracy: 0.7500\n",
            "Epoch 454/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5469 - accuracy: 0.7394 - val_loss: 0.5145 - val_accuracy: 0.7713\n",
            "Epoch 455/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5352 - accuracy: 0.7361 - val_loss: 0.6510 - val_accuracy: 0.7021\n",
            "Epoch 456/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5538 - accuracy: 0.7267 - val_loss: 0.5342 - val_accuracy: 0.7447\n",
            "Epoch 457/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5202 - accuracy: 0.7503 - val_loss: 0.5113 - val_accuracy: 0.7553\n",
            "Epoch 458/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5219 - accuracy: 0.7503 - val_loss: 0.5396 - val_accuracy: 0.7394\n",
            "Epoch 459/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5747 - accuracy: 0.7267 - val_loss: 0.6959 - val_accuracy: 0.6755\n",
            "Epoch 460/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5845 - accuracy: 0.7187 - val_loss: 0.5508 - val_accuracy: 0.7819\n",
            "Epoch 461/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5814 - accuracy: 0.7050 - val_loss: 0.5571 - val_accuracy: 0.7394\n",
            "Epoch 462/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5373 - accuracy: 0.7292 - val_loss: 0.4905 - val_accuracy: 0.7606\n",
            "Epoch 463/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5223 - accuracy: 0.7459 - val_loss: 0.4972 - val_accuracy: 0.7606\n",
            "Epoch 464/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.5214 - accuracy: 0.7405 - val_loss: 0.4845 - val_accuracy: 0.7553\n",
            "Epoch 465/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5164 - accuracy: 0.7575 - val_loss: 0.4983 - val_accuracy: 0.7500\n",
            "Epoch 466/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.5244 - accuracy: 0.7434 - val_loss: 0.5186 - val_accuracy: 0.7500\n",
            "Epoch 467/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5763 - accuracy: 0.7144 - val_loss: 0.5268 - val_accuracy: 0.7606\n",
            "Epoch 468/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5272 - accuracy: 0.7332 - val_loss: 0.5012 - val_accuracy: 0.7713\n",
            "Epoch 469/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5138 - accuracy: 0.7456 - val_loss: 0.5079 - val_accuracy: 0.7553\n",
            "Epoch 470/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5490 - accuracy: 0.7376 - val_loss: 0.5113 - val_accuracy: 0.7606\n",
            "Epoch 471/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5276 - accuracy: 0.7387 - val_loss: 0.5829 - val_accuracy: 0.7181\n",
            "Epoch 472/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5381 - accuracy: 0.7329 - val_loss: 0.5393 - val_accuracy: 0.7447\n",
            "Epoch 473/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5403 - accuracy: 0.7332 - val_loss: 0.5093 - val_accuracy: 0.7713\n",
            "Epoch 474/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5220 - accuracy: 0.7390 - val_loss: 0.5174 - val_accuracy: 0.7500\n",
            "Epoch 475/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5311 - accuracy: 0.7383 - val_loss: 0.5211 - val_accuracy: 0.7287\n",
            "Epoch 476/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5293 - accuracy: 0.7394 - val_loss: 0.4934 - val_accuracy: 0.7819\n",
            "Epoch 477/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5410 - accuracy: 0.7445 - val_loss: 0.5876 - val_accuracy: 0.7713\n",
            "Epoch 478/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5864 - accuracy: 0.7177 - val_loss: 0.5025 - val_accuracy: 0.7819\n",
            "Epoch 479/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5272 - accuracy: 0.7510 - val_loss: 0.4934 - val_accuracy: 0.7500\n",
            "Epoch 480/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5368 - accuracy: 0.7466 - val_loss: 0.5369 - val_accuracy: 0.7660\n",
            "Epoch 481/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5237 - accuracy: 0.7401 - val_loss: 0.5010 - val_accuracy: 0.7926\n",
            "Epoch 482/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5356 - accuracy: 0.7347 - val_loss: 0.5174 - val_accuracy: 0.7553\n",
            "Epoch 483/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5467 - accuracy: 0.7394 - val_loss: 0.4974 - val_accuracy: 0.7553\n",
            "Epoch 484/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5218 - accuracy: 0.7423 - val_loss: 0.4985 - val_accuracy: 0.7500\n",
            "Epoch 485/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5170 - accuracy: 0.7448 - val_loss: 0.5313 - val_accuracy: 0.7606\n",
            "Epoch 486/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5338 - accuracy: 0.7383 - val_loss: 0.5127 - val_accuracy: 0.7713\n",
            "Epoch 487/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5104 - accuracy: 0.7568 - val_loss: 0.5045 - val_accuracy: 0.7713\n",
            "Epoch 488/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5169 - accuracy: 0.7474 - val_loss: 0.5195 - val_accuracy: 0.7500\n",
            "Epoch 489/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5321 - accuracy: 0.7470 - val_loss: 0.5513 - val_accuracy: 0.7181\n",
            "Epoch 490/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5345 - accuracy: 0.7340 - val_loss: 0.5097 - val_accuracy: 0.7553\n",
            "Epoch 491/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5327 - accuracy: 0.7361 - val_loss: 0.5321 - val_accuracy: 0.7447\n",
            "Epoch 492/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5229 - accuracy: 0.7430 - val_loss: 0.5145 - val_accuracy: 0.7872\n",
            "Epoch 493/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5373 - accuracy: 0.7412 - val_loss: 0.5088 - val_accuracy: 0.7394\n",
            "Epoch 494/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5107 - accuracy: 0.7459 - val_loss: 0.4917 - val_accuracy: 0.7553\n",
            "Epoch 495/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5280 - accuracy: 0.7318 - val_loss: 0.5058 - val_accuracy: 0.7713\n",
            "Epoch 496/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.5177 - accuracy: 0.7474 - val_loss: 0.5201 - val_accuracy: 0.7447\n",
            "Epoch 497/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5358 - accuracy: 0.7466 - val_loss: 0.6927 - val_accuracy: 0.7021\n",
            "Epoch 498/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5461 - accuracy: 0.7340 - val_loss: 0.4999 - val_accuracy: 0.7766\n",
            "Epoch 499/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5253 - accuracy: 0.7383 - val_loss: 0.5489 - val_accuracy: 0.7394\n",
            "Epoch 500/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5418 - accuracy: 0.7332 - val_loss: 0.5455 - val_accuracy: 0.7713\n",
            "Epoch 501/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5468 - accuracy: 0.7296 - val_loss: 0.5199 - val_accuracy: 0.7500\n",
            "Epoch 502/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5319 - accuracy: 0.7365 - val_loss: 0.7118 - val_accuracy: 0.7021\n",
            "Epoch 503/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5254 - accuracy: 0.7437 - val_loss: 0.4991 - val_accuracy: 0.7819\n",
            "Epoch 504/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.5083 - accuracy: 0.7445 - val_loss: 0.5397 - val_accuracy: 0.7500\n",
            "Epoch 505/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5322 - accuracy: 0.7369 - val_loss: 0.6103 - val_accuracy: 0.7181\n",
            "Epoch 506/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5406 - accuracy: 0.7369 - val_loss: 0.5461 - val_accuracy: 0.7606\n",
            "Epoch 507/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5386 - accuracy: 0.7285 - val_loss: 0.5684 - val_accuracy: 0.7394\n",
            "Epoch 508/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5169 - accuracy: 0.7514 - val_loss: 0.5187 - val_accuracy: 0.7553\n",
            "Epoch 509/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5108 - accuracy: 0.7488 - val_loss: 0.4853 - val_accuracy: 0.7447\n",
            "Epoch 510/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5140 - accuracy: 0.7477 - val_loss: 0.5221 - val_accuracy: 0.7340\n",
            "Epoch 511/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5060 - accuracy: 0.7539 - val_loss: 0.5331 - val_accuracy: 0.7287\n",
            "Epoch 512/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5331 - accuracy: 0.7383 - val_loss: 0.5149 - val_accuracy: 0.7606\n",
            "Epoch 513/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5135 - accuracy: 0.7474 - val_loss: 0.5616 - val_accuracy: 0.7234\n",
            "Epoch 514/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5396 - accuracy: 0.7376 - val_loss: 0.5011 - val_accuracy: 0.7553\n",
            "Epoch 515/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5170 - accuracy: 0.7430 - val_loss: 0.5831 - val_accuracy: 0.7128\n",
            "Epoch 516/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5268 - accuracy: 0.7423 - val_loss: 0.5117 - val_accuracy: 0.7553\n",
            "Epoch 517/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5242 - accuracy: 0.7434 - val_loss: 0.5166 - val_accuracy: 0.7340\n",
            "Epoch 518/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5260 - accuracy: 0.7430 - val_loss: 0.5165 - val_accuracy: 0.7553\n",
            "Epoch 519/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5078 - accuracy: 0.7604 - val_loss: 0.4853 - val_accuracy: 0.7553\n",
            "Epoch 520/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.5089 - accuracy: 0.7510 - val_loss: 0.5450 - val_accuracy: 0.7447\n",
            "Epoch 521/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5146 - accuracy: 0.7572 - val_loss: 0.5229 - val_accuracy: 0.7660\n",
            "Epoch 522/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5053 - accuracy: 0.7568 - val_loss: 0.5005 - val_accuracy: 0.7287\n",
            "Epoch 523/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5076 - accuracy: 0.7528 - val_loss: 0.5105 - val_accuracy: 0.7553\n",
            "Epoch 524/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5048 - accuracy: 0.7528 - val_loss: 0.5112 - val_accuracy: 0.7660\n",
            "Epoch 525/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5066 - accuracy: 0.7561 - val_loss: 0.4996 - val_accuracy: 0.7447\n",
            "Epoch 526/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5003 - accuracy: 0.7608 - val_loss: 0.5022 - val_accuracy: 0.7394\n",
            "Epoch 527/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5067 - accuracy: 0.7459 - val_loss: 0.5312 - val_accuracy: 0.7340\n",
            "Epoch 528/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5254 - accuracy: 0.7365 - val_loss: 0.5416 - val_accuracy: 0.7447\n",
            "Epoch 529/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5418 - accuracy: 0.7379 - val_loss: 0.5333 - val_accuracy: 0.7394\n",
            "Epoch 530/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5051 - accuracy: 0.7517 - val_loss: 0.5513 - val_accuracy: 0.7340\n",
            "Epoch 531/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.5171 - accuracy: 0.7485 - val_loss: 0.4911 - val_accuracy: 0.7394\n",
            "Epoch 532/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.4970 - accuracy: 0.7528 - val_loss: 0.5024 - val_accuracy: 0.7500\n",
            "Epoch 533/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5048 - accuracy: 0.7477 - val_loss: 0.4935 - val_accuracy: 0.7447\n",
            "Epoch 534/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5084 - accuracy: 0.7524 - val_loss: 0.5270 - val_accuracy: 0.7394\n",
            "Epoch 535/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5094 - accuracy: 0.7521 - val_loss: 0.5122 - val_accuracy: 0.7660\n",
            "Epoch 536/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5263 - accuracy: 0.7427 - val_loss: 0.5074 - val_accuracy: 0.7553\n",
            "Epoch 537/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5236 - accuracy: 0.7470 - val_loss: 0.5173 - val_accuracy: 0.7447\n",
            "Epoch 538/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5205 - accuracy: 0.7427 - val_loss: 0.5190 - val_accuracy: 0.7447\n",
            "Epoch 539/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5270 - accuracy: 0.7318 - val_loss: 0.5079 - val_accuracy: 0.7606\n",
            "Epoch 540/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5217 - accuracy: 0.7369 - val_loss: 0.5285 - val_accuracy: 0.7394\n",
            "Epoch 541/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5322 - accuracy: 0.7285 - val_loss: 0.5170 - val_accuracy: 0.7713\n",
            "Epoch 542/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.5278 - accuracy: 0.7401 - val_loss: 0.5135 - val_accuracy: 0.7340\n",
            "Epoch 543/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5297 - accuracy: 0.7398 - val_loss: 0.4945 - val_accuracy: 0.7553\n",
            "Epoch 544/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5012 - accuracy: 0.7608 - val_loss: 0.5014 - val_accuracy: 0.7447\n",
            "Epoch 545/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.4991 - accuracy: 0.7575 - val_loss: 0.4933 - val_accuracy: 0.7500\n",
            "Epoch 546/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5149 - accuracy: 0.7492 - val_loss: 0.4875 - val_accuracy: 0.7447\n",
            "Epoch 547/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.5165 - accuracy: 0.7481 - val_loss: 0.5084 - val_accuracy: 0.7606\n",
            "Epoch 548/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5049 - accuracy: 0.7521 - val_loss: 0.4995 - val_accuracy: 0.7553\n",
            "Epoch 549/8000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5125 - accuracy: 0.7499 - val_loss: 0.4972 - val_accuracy: 0.7500\n",
            "Epoch 550/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.4969 - accuracy: 0.7691 - val_loss: 0.5125 - val_accuracy: 0.7340\n",
            "Epoch 551/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5007 - accuracy: 0.7517 - val_loss: 0.5658 - val_accuracy: 0.7394\n",
            "Epoch 552/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5185 - accuracy: 0.7561 - val_loss: 0.5046 - val_accuracy: 0.7660\n",
            "Epoch 553/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.4995 - accuracy: 0.7579 - val_loss: 0.5065 - val_accuracy: 0.7394\n",
            "Epoch 554/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5010 - accuracy: 0.7543 - val_loss: 0.5116 - val_accuracy: 0.7553\n",
            "Epoch 555/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5143 - accuracy: 0.7485 - val_loss: 0.5358 - val_accuracy: 0.7447\n",
            "Epoch 556/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5133 - accuracy: 0.7430 - val_loss: 0.4887 - val_accuracy: 0.7713\n",
            "Epoch 557/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5113 - accuracy: 0.7463 - val_loss: 0.5354 - val_accuracy: 0.7340\n",
            "Epoch 558/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.5430 - accuracy: 0.7184 - val_loss: 0.5222 - val_accuracy: 0.7660\n",
            "Epoch 559/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.5148 - accuracy: 0.7499 - val_loss: 0.5184 - val_accuracy: 0.7447\n",
            "Epoch 560/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5045 - accuracy: 0.7553 - val_loss: 0.5313 - val_accuracy: 0.7394\n",
            "Epoch 561/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5058 - accuracy: 0.7564 - val_loss: 0.5401 - val_accuracy: 0.7713\n",
            "Epoch 562/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.5251 - accuracy: 0.7416 - val_loss: 0.5267 - val_accuracy: 0.7500\n",
            "Epoch 563/8000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.5091 - accuracy: 0.7441 - val_loss: 0.5223 - val_accuracy: 0.7553\n",
            "Epoch 564/8000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5061 - accuracy: 0.7604 - val_loss: 0.5045 - val_accuracy: 0.7660\n",
            "Epoch 564: early stopping\n"
          ]
        }
      ],
      "source": [
        "history=model.fit(XTRAIN, YTRAIN,validation_data=(XTEST,YTEST),epochs=8000,batch_size=108,callbacks=[callback1,callback2])"
      ],
      "id": "3487115c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "4db5d3c3",
        "outputId": "057ac585-09d0-4e37-8ca9-c3ab2d180a6f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the Learning Curve\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()"
      ],
      "id": "4db5d3c3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "4Pxqw15XA_1s",
        "outputId": "fa802fae-13fe-441c-90f7-ab46289c801e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.savefig('plot.png')"
      ],
      "id": "4Pxqw15XA_1s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "bvgpy9tXF4vN",
        "outputId": "1d081c96-f537-4ff0-f7d4-43bf560139d0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEKCAYAAAAVaT4rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAcSElEQVR4nO3df5Akd3nf8ffT3TOze7f3+1aX4yQ4SQi7ZMf6URsVCpSqLCJHxoml2AqWi3KuUqpSmUAMReIg4kpiUk4VOBUDckjssxEcAVsigJDi2ATlkEVcBqEVSEI/jHU+JHOHpNvT3elu73ZnZ6af/NHfmZ3Z3dEtp+2Zue7Pq2pre3p+Pds9+9lnv/OdbnN3RESkPKJhFyAiIoOl4BcRKRkFv4hIySj4RURKRsEvIlIyCn4RkZJJ8nxwM3sOOAW0gKa7T5nZVuAeYDfwHPAOdz+eZx0iIrJoEB3/T7v7le4+FS7fAex398uA/eGyiIgMyDCGem4C9oXlfcDNQ6hBRKS0LM9P7prZ94HjgAO/7+57zeyEu28O1xtwvH25n+3bt/vu3btzq1NEpIgeffTRo+4+uXR9rmP8wFvd/bCZXQA8YGZ/1X2lu7uZrfiXx8xuB24HeP3rX8/09HTOpYqIFIuZPb/S+lyHetz9cPh+BLgXuAZ4ycx2hqJ2Akf63Hevu0+5+9Tk5LI/WCIico5yC34zW29mG9rLwM8ATwL3A3vCzfYA9+VVg4iILJfnUM8O4N5sGJ8E+CN3/4qZPQJ83sxuA54H3pFjDSIiskRuwe/uB4ErVlj/MvC2vJ5XRERenT65KyJSMgp+EZGSUfCLiJRMoYP/S98+xGe/ueI0VhGR0ip08P+vx3/IPY/8YNhliIiMlEIHfxxFtFKdTF5EpFuhgz+JTMEvIrJEoYM/jo1mmg67DBGRkVLo4FfHLyKyXKGDP46MpoJfRKRHsYPf1PGLiCxV6OBPYnX8IiJLFTr448hIFfwiIj0KHfxJFKnjFxFZotDBH2tWj4jIMoUO/iTSPH4RkaUKHfzq+EVElit08Ceaxy8iskyhgz+KDHc0s0dEpEuhgz+JDEBdv4hIl0IHfxxlP17qCn4RkbZCB786fhGR5Qod/HEI/lZLwS8i0lbo4E/idsevufwiIm2FDv5Ox6+hHhGRjkIHv8b4RUSWK3TwR6aOX0RkqUIHf3uMX8EvIrKo0MHfnsevoR4RkUWFDv5Eb+6KiCxT6OCPI03nFBFZqtDBr45fRGS5Qgd/rOmcIiLL5B78Zhab2XfM7E/C5YvN7GEzO2Bm95hZNa/n1ge4RESWG0TH/17gma7LHwE+6u5vBI4Dt+X1xJ2OX8fqERHpyDX4zexC4OeAPwyXDbge+EK4yT7g5ryeP9FhmUVElsm74/8Y8G+A9rSabcAJd2+Gy4eAXXk9ucb4RUSWyy34zewfAUfc/dFzvP/tZjZtZtMzMzPnVMPirB5N5xQRacuz438L8PNm9hxwN9kQz8eBzWaWhNtcCBxe6c7uvtfdp9x9anJy8pwK0Bi/iMhyuQW/u3/Q3S90993ArcDX3P2dwIPALeFme4D78qpBx+oREVluGPP4PwC838wOkI35fzKvJ9JhmUVElkvOfpPXzt3/HPjzsHwQuGYQz6vDMouILFfoT+62p3Mq+EVEFhU6+GON8YuILFPo4NcYv4jIcoUO/ljz+EVElil08KvjFxFZrtDBr6NziogsV4rgV8cvIrKoFMGvjl9EZFGhg1/z+EVElit08IeGX0M9IiJdCh38ZkYSmaZzioh0KXTwQzbOr45fRGRR4YM/iYyWjscvItJR+OBXxy8i0qsUwa9ZPSIii0oQ/BEtV/CLiLQVPvg1xi8i0qvwwa8xfhGRXoUP/iTWPH4RkW6FD351/CIivQof/Ilm9YiI9Ch88Eemjl9EpFvhgz8b41fwi4i0FT744yhS8IuIdCl88GuMX0SkV+GDP5vVo+mcIiJthQ9+dfwiIr0KH/yaxy8i0qvwwa+OX0SkV+GDP46Mpg7SJiLSUYrgV8cvIrKo8MGf6Hj8IiI9Ch/86vhFRHrlFvxmNmZm3zKzx83sKTP7UFh/sZk9bGYHzOweM6vmVQNkb+5qHr+IyKI8O/46cL27XwFcCdxoZm8GPgJ81N3fCBwHbsuxhqzj15u7IiIduQW/Z2bDxUr4cuB64Ath/T7g5rxqgOwgbZrHLyKyKNcxfjOLzewx4AjwAPA3wAl3b4abHAJ25VlDZBrjFxHplmvwu3vL3a8ELgSuAX58tfc1s9vNbNrMpmdmZs65hkSf3BUR6TGQWT3ufgJ4ELgW2GxmSbjqQuBwn/vsdfcpd5+anJw85+eOo4hUwS8i0pHnrJ5JM9sclseBG4BnyP4A3BJutge4L68aQGP8IiJLJWe/yTnbCewzs5jsD8zn3f1PzOxp4G4z+y3gO8Anc6xB8/hFRJbILfjd/QngqhXWHyQb7x8IzeMXEelVik/upo7G+UVEgsIHfxIZgI7XIyISFD74o3bwq+MXEQFKEPztjl8ze0REMoUP/jjKfkR1/CIimcIHf6KhHhGRHoUP/rgz1KMpnSIiUILgV8cvItKr8MHf6fh1TH4REaBEwa+OX0Qks6rgN7P1ZhaF5TeZ2c+bWSXf0tZGrOmcIiI9Vtvxfx0YM7NdwFeBXwE+nVdRaykJ0zlTfXJXRARYffCbu58BfgH4b+7+T4GfyK+staMxfhGRXqsOfjO7Fngn8L/DujifktaWZvWIiPRabfC/D/ggcK+7P2Vml5CdUGXkxbHm8YuIdFvV8fjd/SHgIYDwJu9Rd/+1PAtbK+r4RUR6rXZWzx+Z2UYzWw88CTxtZr+eb2lrQ7N6RER6rXao53J3PwncDPwZcDHZzJ6RF5s6fhGRbqsN/kqYt38zcL+7N4DzIkmTWB2/iEi31Qb/7wPPAeuBr5vZG4CTeRW1ltqHZdapF0VEMqt9c/dO4M6uVc+b2U/nU9La0olYRER6rSr4zWwT8B+A68Kqh4D/CLySU11r46H/zM5jR4HraGk6p4gIsPqhnruAU8A7wtdJ4FN5FbVmDj3C+h/+JaCOX0SkbVUdP3Cpu/9i1+UPmdljeRS0pqIES5uAZvWIiLSttuOfM7O3ti+Y2VuAuXxKWkNRjHkW/DpWj4hIZrUd/68Cnwlj/QDHgT35lLSGogRLW4A6fhGRttXO6nkcuMLMNobLJ83sfcATeRb3msWVTsff0mGZRUSAH/EMXO5+MnyCF+D9OdSztro6fr25KyKSeS2nXrQ1qyIvXWP8rZamc4qIwGsL/tFvoaME1PGLiPR41TF+MzvFygFvwHguFa2lKOl0/A3N6hERAc4S/O6+YVCF5CJKIG0Hv4Z6RETgtQ31jL4ohrSFmYJfRKQtt+A3s4vM7EEze9rMnjKz94b1W83sATN7NnzfklcN7U/uVuOIhaaCX0QE8u34m8C/cvfLgTcD7zazy4E7gP3ufhmwP1zORxjqqcYRC+r4RUSAHIPf3V9w92+H5VPAM8Au4CZgX7jZPrKTu+QjSsBTarGGekRE2gYyxm9mu4GrgIeBHe7+QrjqRWBHn/vcbmbTZjY9MzNzbk8cxQDUYjTUIyIS5B78ZjYBfBF4X9enfgFwd6fP5wHcfa+7T7n71OTk5Lk9eZRNWlqXuKZziogEuQZ/OE/vF4HPufuXwuqXzGxnuH4ncCS3AkLwj8WpOn4RkSDPWT0GfBJ4xt1/p+uq+1k8suce4L68algMfvTmrohIsNrDMp+LtwC/Any366Qt/xb4MPB5M7sNeJ7sjF75CME/ro5fRKQjt+B397+g/4Hc3pbX8/bodPzOrDp+ERGg8J/czYK/Frk6fhGRoBTBPxa75vGLiASlCP5alFJXxy8iAhQ++Nsf4FLHLyLSVvDg7xrjV/CLiAClCf4WjaY+uSsiAiUJ/qo6fhGRjoIHfzbGX42cht7cFREBCh/8XbN61PGLiABlCX5r0WilZAcDFREpt1IEfzVy3KGZKvhFREoR/BXLhnk0l19EpOjBH4eOPwS/jtcjIlL04G93/FEIfnX8IiLlCH51/CIii0oR/ElnjF9v7oqIFDz4sw9wVdTxi4h0FDz427N6WoBm9YiIQEmCvz3Uo2Pyi4iUJPgraB6/iEhbwYM/G+NPNMYvItJR8OBvH7IhC/wzC61hViMiMhJKEfxjoeM/XW8OsxoRkZFQiuBvd/yzCn4REQW/iEjZFDv4zcBiElKqccSpeQW/iEixgx+yrj9tMjGWMFtvDLsaEZGhK0/w1xJm1fGLiJQl+FtZ8Nc1nVNEpATBH2uoR0SkSwmCPxvq2VBLNKtHRIQSBf/EmMb4RUSgDMGfVKE5H8b4FfwiIrkFv5ndZWZHzOzJrnVbzewBM3s2fN+S1/N3VDfAwmkmaonm8YuIkG/H/2ngxiXr7gD2u/tlwP5wOV/V9VA/xUQtod5MdWhmESm93ILf3b8OHFuy+iZgX1jeB9yc1/N31Cayjn8sO3yDDtQmImU36DH+He7+Qlh+EdjR74ZmdruZTZvZ9MzMzLk/Y3UCFmZZX8uCX8M9IlJ2Q3tz190d8Fe5fq+7T7n71OTk5Lk/UW0C6rOsq2YnZZlv6ENcIlJugw7+l8xsJ0D4fiT3Z6xmQz3jlSz4dTIWESm7QQf//cCesLwHuC/3Z6xOwMIpxivZjzqnjl9ESi7P6Zx/DHwD+DEzO2RmtwEfBm4ws2eBfxAu56s2AZ6yLsoO1zCnjl9ESi7J64Hd/Zf7XPW2vJ5zRdUJACaYBzTUIyJS/E/uhuBfF4JfQz0iUnbFD/5aFvxjfgaAuQVN5xSRcit+8FfXAzDuc4A6fhGREgT/BgBqrTmuix7n6u99bMgFiYgMV25v7o6MMNQTNWb5TPUjcBjgd4dakojIMJWg48+Cn4XZxXXe9wPDIiKFV/zgDx0/xw4urmvOD6cWEZERUPzgH98CP/Z2+Iuusf3G3PDqEREZsuIHP8A/vpOe48E1zgytFBGRYStH8E8sObqnOn4RKbFyBD/A+NbFZQW/iJRYeYL/J39hcVnBLyIlVp7gv/HDfHnHe7JljfGLSImVJ/jjCoc3XpUtq+MXkRIrT/ADlfFsTn+6oI5fRMqrVMG/ZeNGAE6fPjXkSkREhqdUwb91y2YATp06OeRKRESGp1TBvy0E/+lZdfwiUl6lCv4dWzcBcObM7FluKSJSXKUK/skN48x7hbqCX0RKrFTBn8QRdauxMH962KWIiAxNqYIfoBGN0aor+EWkvEoX/K14jFZd8/hFpLxKF/xeGccXzuA6C5eIlFTpgj+qrqfqdY7OLgy7FBGRoShd8MdjE2ywOQ4d13CPiJRT6YI/2fQ6LrDjHDquA7WJSDklwy5g0Ma3XcQExzn8sj69KyLlVLqOv7r1ImJzXn7pb4ddiojIUJQu+Nm4C4Dnv/+sZvaISCmVMPhfB0Bl9gUOHtUHuUSkfMoX/Juyjv/v2Mvsfeigun4RKZ3SvbnL2GaorOPf8Tk+9tgcH//bS7n0wtexees2ammd5rptJHFMHBmxGbS/sPAA2WUL6yyKsqux8D3CIgMimuNbSeqvYKRY+65pStycJ03GSJPx7HtcI41qYIa70zhzAmobqMYxsTmRQewLJK15zFvMJ5tokNBstfD6LFabIIoikigiisAdUndaqZO64w5jlZhqEuEOztn/2Fnn513huuY8ldlDEFdI4xoe1/B4DE9qYKGX8BSzUAwQn3mJeP4YrbGtpNWNRI3TGGm2bnw7rfHtYTsT7rNYY/YQ2XVuvXW1fx737O5JlO0bd3DPflJ38LQBrQYejy0+z9Kfy3p/9sXLvdfT93pb8fZLH2/Z853tfq15iGvZ7QwiM8wdTxvMtyLmm04cGWOViGoc9X289mOm7pyuL7CwUIeoku0zTyFtQFTFmvNYukBa3dhz528ePMYXv32I99/wJmbrTS7YUGP7RA2AKMqqtXZ9oYCo6/6pO2nLaaYtWm40G3W8UceTMSqVam+d3v61s/AraES2uE3ar2FP03CHxR7WrHebdx42bdF85YfMVzcTV9eRxBFR/5d5zzajOUe0MEva/TpdYfuufH/aL8KszmUvhFe//9Z1VZJ4bXt0G0bHa2Y3Ah8HYuAP3f3Dr3b7qakpn56eXrsCvvEJ/On7sR98c+0e8zVK3ZinSoUmFct+MWLrv2/qXsGBMWtQ9wqtAf7z1q6xX11GStVanPYaYyzQIqLa5/ZtqRvRq/y8/e4Di38ivBMKi+sdo06FjZZN313wGCciIiVq/0Huuu3S+7Lk8tLruy+Ddd2vff3yy5zl+u7HjUnZacc44zVOso4xFqjRwHDGrEHqxhxV5qhRp9K5X1tESkLKOHUA5qmQErGVU5192PTstZNY2rMfGh5jOImlND3Kmg3insc1UhaosEDCAhXqXiHCucCOU6dCnSrzXiWxFjEp65ljvdVpekRiaafObL8YMWln/ZxXeYX1TDDHSdZR9wo1a1ClSY0GNRZ6Xlep926/NOy/9rqIlJo1ATjjtVW0P3T21jrLtl/dE/oldYuIFhEe9lscXmMxac/vct2TJfvc6H4NL31NHX/nV3jDm65YZbVLajd71N2nlq4feMdvZjHwCeAG4BDwiJnd7+5PD6yIa9+NXftuOP0yzB1j9uQxTp54mabV8NNHST2llRqpp7gv/jp2/kZ62ulKPWsts53UWXbMW1Tnj9KobiGN4sX7YrSSdVhrnqQ5R9SaJ27NE7fqxK050qgC41uIFk7RdCMl69JbUZVGPIZjjDdOUEnnic1pjW0lnj+Gu5OGLjfrtkKHFLqLViul5e0KXps0qjC74VLwlCitE7fqRKH+uFXHLSKNKsTNM7TiMSJvUK9tZ37dTqr1YySNWRrJegAa1U1U68ep1V8GLHT04cXf7nw9K9q6/hPo/AzuoasL3X3qnW4we6QWcWuBF2ub8ahCpXEq20cW4RZn3Vf7cRd3cO9y+3JnH4burbNHffH1QFed3f+5tB+v637g4bb0uX22/N3xXSTN01SapzgRjdGKqgAsJBPUaFL1eaLWPDTmSb33+dLwc87E47hDktaJvMmx8W14dYLIm1jaxPBsX6UNmvE4HiVU68egvZ08pWpNto8bM6cWmBivcmoBGi3HvEncnCdKF4jSBZruHBi7gChtZK/ttM68Zf9ZHE/GaFQ3kXgTT8bwuEbUmscac9n2szjbN0TEzdNUG69wMp6g0jxJ1GowF9U4HddoRRXSqEYaVbP/Cry18j70tLN/3CLm1+9iXeskcf3k4rZ6Fe1bLFQ20YrHGKvPdL/6Otebhz813sqeOmy37HuU7Qfi7PWYNrLX7dJ93nntdL3ugF1btp+1zh/VMIZ6rgEOuPtBADO7G7gJGFzwt63fBuu3MbEdJgb+5CLnpw3h+wVDrUJei2G8ubsL+EHX5UNhXQ8zu93Mps1semZmZmDFiYgU3cjO6nH3ve4+5e5Tk5OTwy5HRKQwhhH8h4GLui5fGNaJiMgADCP4HwEuM7OLzawK3ArcP4Q6RERKaeBv7rp708zeA/wfsumcd7n7U4OuQ0SkrIbyAS53/1PgT4fx3CIiZTeyb+6KiEg+FPwiIiUzlEM2/KjMbAZ4/hzvvh04uobl5OV8qFM1rp3zoc7zoUY4P+ocVo1vcPdl8+HPi+B/LcxseqVjVYya86FO1bh2zoc6z4ca4fyoc9Rq1FCPiEjJKPhFREqmDMG/d9gFrNL5UKdqXDvnQ53nQ41wftQ5UjUWfoxfRER6laHjFxGRLoUOfjO70cy+Z2YHzOyOYdfTZmbPmdl3zewxM5sO67aa2QNm9mz4vmUIdd1lZkfM7MmudSvWZZk7w7Z9wsyuHmKNv2lmh8P2fMzM3t513QdDjd8zs384oBovMrMHzexpM3vKzN4b1o/atuxX58hsTzMbM7NvmdnjocYPhfUXm9nDoZZ7wnG/MLNauHwgXL877xrPUuenzez7XdvyyrB+KPu8w90L+UV2HKC/AS4BqsDjwOXDrivU9hywfcm63wbuCMt3AB8ZQl3XAVcDT56tLuDtwJ+RnY7ozcDDQ6zxN4F/vcJtLw/7vQZcHF4P8QBq3AlcHZY3AH8dahm1bdmvzpHZnmGbTITlCvBw2EafB24N638PeFdY/hfA74XlW4F7BrQt+9X5aeCWFW4/lH3e/ipyx98505e7LwDtM32NqpuAfWF5H3DzoAtw968Dx5as7lfXTcBnPPNNYLOZ7RxSjf3cBNzt7nV3/z5wgOx1kSt3f8Hdvx2WTwHPkJ1saNS2Zb86+xn49gzbZDZcrIQvB64HvhDWL92W7W38BeBtZmc7HXqudfYzlH3eVuTgX9WZvobEga+a2aNmdntYt8PdXwjLLwI7hlPaMv3qGrXt+57wL/NdXcNkQ68xDDVcRdYBjuy2XFInjND2NLPYzB4DjgAPkP2nccLdmyvU0akxXP8KsC3vGleq093b2/I/hW35UTOrLa0zGOg+L3Lwj7K3uvvVwM8C7zaz67qv9Ox/wZGbbjWqdQH/HbgUuBJ4Afgvwy0nY2YTwBeB97n7ye7rRmlbrlDnSG1Pd2+5+5VkJ226BvjxYdbTz9I6zewngQ+S1fv3gK3AB4ZYYkeRg39kz/Tl7ofD9yPAvWQv5pfa/+qF70eGV2GPfnWNzPZ195fCL10K/AGLww9Dq9HMKmRh+jl3/1JYPXLbcqU6R3F7hrpOAA8C15INjbQPK99dR6fGcP0m4OVB1bikzhvDcJq7ex34FCOyLYsc/CN5pi8zW29mG9rLwM8AT5LVtifcbA9w33AqXKZfXfcD/yzMTngz8ErXMMZALRkb/Sdk2xOyGm8NMz0uBi4DvjWAegz4JPCMu/9O11UjtS371TlK29PMJs1sc1geB24gey/iQeCWcLOl27K9jW8Bvhb+u8pVnzr/qusPvZG9D9G9LYf3+zPId5IH/UX2zvlfk40J/saw6wk1XUI2M+Jx4Kl2XWTjkPuBZ4H/C2wdQm1/TPavfYNszPG2fnWRzUb4RNi23wWmhljj/wg1PEH2C7Wz6/a/EWr8HvCzA6rxrWTDOE8Aj4Wvt4/gtuxX58hsT+CngO+EWp4E/n1YfwnZH50DwP8EamH9WLh8IFx/yYC2Zb86vxa25ZPAZ1mc+TOUfd7+0id3RURKpshDPSIisgIFv4hIySj4RURKRsEvIlIyCn4RkZJR8EtpmVmr66iJj9kaHsHVzHZb1xFERUZJcvabiBTWnGcfsRcpFXX8IktYdr6E37bsnAnfMrM3hvW7zexr4YBb+83s9WH9DjO7NxyL/XEz+/vhoWIz+4NwfPavhk90Yma/Ztkx8J8ws7uH9GNKiSn4pczGlwz1/FLXda+4+98F/ivwsbDud4F97v5TwOeAO8P6O4GH3P0KsnMFPBXWXwZ8wt1/AjgB/GJYfwdwVXicX83rhxPpR5/cldIys1l3n1hh/XPA9e5+MBzE7EV332ZmR8kOX9AI619w9+1mNgNc6NmBuNqPsZvs0LyXhcsfACru/ltm9hVgFvgy8GVfPI67yECo4xdZmfdZ/lHUu5ZbLL6n9nNkx2m5Gnik6yiTIgOh4BdZ2S91ff9GWP5LsqO8ArwT+H9heT/wLuicjGNTvwc1swi4yN0fJDs2+yZg2X8dInlSpyFlNh7OmNT2FXdvT+ncYmZPkHXtvxzW/UvgU2b268AM8M/D+vcCe83sNrLO/l1kRxBdSQx8NvxxMOBOz47fLjIwGuMXWSKM8U+5+9Fh1yKSBw31iIiUjDp+EZGSUccvIlIyCn4RkZJR8IuIlIyCX0SkZBT8IiIlo+AXESmZ/w/GIXsdBr0X6gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()"
      ],
      "id": "bvgpy9tXF4vN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14e30691"
      },
      "outputs": [],
      "source": [
        "#load best_model\n",
        "model.load_weights('best_amp.hdf5')"
      ],
      "id": "14e30691"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74c54989",
        "outputId": "17185cea-0ff5-416c-ad11-42328e1948dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 0s 3ms/step - loss: 0.5411 - accuracy: 0.6436\n",
            "accuracy\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on Test data\n",
        "scores=model.evaluate(XTEST,YTEST)\n",
        "print(model.metrics_names[1])"
      ],
      "id": "74c54989"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93680be6",
        "outputId": "ce8f1784-4f7d-4d05-b456-57d1e7091dfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 0s 1ms/step\n",
            "[[0.70584893]\n",
            " [0.91693217]\n",
            " [0.8626834 ]\n",
            " [0.8334258 ]\n",
            " [0.7119546 ]\n",
            " [0.9288001 ]\n",
            " [0.46895447]\n",
            " [0.46895447]\n",
            " [0.46895447]\n",
            " [0.82838523]\n",
            " [0.88189113]\n",
            " [0.46895447]\n",
            " [0.46895447]\n",
            " [0.44282383]\n",
            " [0.16733362]\n",
            " [0.7043952 ]\n",
            " [0.94409263]\n",
            " [0.22906263]\n",
            " [0.22869913]\n",
            " [0.99995375]]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ],
      "source": [
        "# Make prediction\n",
        "prediction=model.predict(XTRAIN)\n",
        "print(prediction[0:20])\n",
        "print(YTRAIN[0:20])"
      ],
      "id": "93680be6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7uztL-K3r-I",
        "outputId": "f5107968-d5cb-42ff-d0cc-465601c930b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 0s 3ms/step\n",
            "[[0.40576246]\n",
            " [0.59902847]\n",
            " [0.7759308 ]\n",
            " [0.93539995]\n",
            " [0.75427157]\n",
            " [0.83101785]\n",
            " [0.42672387]\n",
            " [0.9436642 ]\n",
            " [0.2157657 ]\n",
            " [0.93952656]\n",
            " [0.5629484 ]\n",
            " [0.976259  ]\n",
            " [0.2563613 ]\n",
            " [0.8892069 ]\n",
            " [0.84744364]\n",
            " [0.3275278 ]\n",
            " [0.85180837]\n",
            " [0.6448938 ]\n",
            " [0.8632253 ]\n",
            " [0.9917295 ]]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ],
      "source": [
        "# Make prediction\n",
        "prediction=model.predict(XTEST)\n",
        "print(prediction[0:20])\n",
        "print(YTEST[0:20])"
      ],
      "id": "z7uztL-K3r-I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "zNxsh7uS-Qkl",
        "outputId": "d680a737-2ef1-4cac-d6da-d76660b4e292"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-31794b83-cff5-4c4d-a500-bb6e0acbe758\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>R</th>\n",
              "      <th>N</th>\n",
              "      <th>D</th>\n",
              "      <th>C</th>\n",
              "      <th>E</th>\n",
              "      <th>Q</th>\n",
              "      <th>G</th>\n",
              "      <th>H</th>\n",
              "      <th>I</th>\n",
              "      <th>...</th>\n",
              "      <th>VHSE4</th>\n",
              "      <th>VHSE5</th>\n",
              "      <th>VHSE6</th>\n",
              "      <th>VHSE7</th>\n",
              "      <th>VHSE8</th>\n",
              "      <th>Z1</th>\n",
              "      <th>Z2</th>\n",
              "      <th>Z3</th>\n",
              "      <th>Z4</th>\n",
              "      <th>Z5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.701667</td>\n",
              "      <td>1.096667</td>\n",
              "      <td>0.471667</td>\n",
              "      <td>0.201667</td>\n",
              "      <td>-0.135</td>\n",
              "      <td>-0.806667</td>\n",
              "      <td>2.35</td>\n",
              "      <td>-1.04</td>\n",
              "      <td>2.065</td>\n",
              "      <td>-0.558333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 109 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31794b83-cff5-4c4d-a500-bb6e0acbe758')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-31794b83-cff5-4c4d-a500-bb6e0acbe758 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-31794b83-cff5-4c4d-a500-bb6e0acbe758');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     A     R    N    D    C    E    Q    G    H    I  ...     VHSE4     VHSE5  \\\n",
              "7  0.0  0.17  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.701667  1.096667   \n",
              "\n",
              "      VHSE6     VHSE7  VHSE8        Z1    Z2    Z3     Z4        Z5  \n",
              "7  0.471667  0.201667 -0.135 -0.806667  2.35 -1.04  2.065 -0.558333  \n",
              "\n",
              "[1 rows x 109 columns]"
            ]
          },
          "execution_count": 194,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[7:8]"
      ],
      "id": "zNxsh7uS-Qkl"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}